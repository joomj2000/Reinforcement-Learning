{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joomj2000/Reinforcement-Learning/blob/main/custom_gym_shooting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive"
      ],
      "metadata": {
        "id": "ChYhyv_FP9hq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KRmGInh2QF55",
        "outputId": "02c86ae2-1bef-4b0c-ed65-7223f7f85cc9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ynuo4qodQbd1",
        "outputId": "2b4e7bd0-f4d9-4a8d-8c2f-00d064f03d1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r /content/drive/MyDrive/SKT_강화학습/gym_examples /content"
      ],
      "metadata": {
        "id": "47nFL13DQNeP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ilSSF7yBmgj_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls gym_examples"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sV2Mt4mZQgfb",
        "outputId": "1c4c1292-b74c-4a6a-aca8-fc689da193d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "envs  __init__.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gymnasium"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gyl30cqxgxbk",
        "outputId": "45f95c46-34d4-4f93-8cbe-4d6698ae471c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gymnasium\n",
            "  Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.25.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.12.2)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium)\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Installing collected packages: farama-notifications, gymnasium\n",
            "Successfully installed farama-notifications-0.0.4 gymnasium-0.29.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "env = gym.make('gym_examples:gym_examples/ShootingAirplane-v0', render_mode='text')"
      ],
      "metadata": {
        "id": "bsIXEH5vPeUB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "obs, info = env.reset()"
      ],
      "metadata": {
        "id": "uYCQ5y0fP0mO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env.render()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kLJMo4TSQrIy",
        "outputId": "5dfedf09-e35a-4399-87a1-dabcfb975af6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         |         \n",
            "         |         \n",
            "         |         \n",
            "         |      H  \n",
            "         |    H H  \n",
            "         |    HHHH \n",
            "         |    H H  \n",
            "         |      H  \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "obs, reward, done,_, info = env.step((0, 0))"
      ],
      "metadata": {
        "id": "LwsoJLX0nTMC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env.render()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sZegK_bTnhr0",
        "outputId": "06d320b5-eb0f-4c1e-ceab-4d627eec622a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "M        |         \n",
            "         |         \n",
            "         |         \n",
            "         |      H  \n",
            "         |    H H  \n",
            "         |    HHHH \n",
            "         |    H H  \n",
            "         |      H  \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "obs, reward, done, _,info = env.step((5, 5))"
      ],
      "metadata": {
        "id": "rnSgMbtHnppw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env.render()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ah-LJysvn0sq",
        "outputId": "27028854-9bf1-4dd0-89f8-cf66872a524b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         |         \n",
            "         |         \n",
            "         |   H     \n",
            "         | HHHHH   \n",
            "         |   H     \n",
            "    MM   |  HHH    \n",
            "         |         \n",
            "    M    |         \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "obs, reward, done,_, info = env.step((3, 3))\n",
        "env.render()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Q2T3wb5n4tL",
        "outputId": "e65dbcd4-0414-4716-f3e9-2fb612ec2f07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         |         \n",
            "         |         \n",
            "         |   H     \n",
            "   H     | HHHHH   \n",
            "         |   H     \n",
            "    MM   |  HHH    \n",
            "         |         \n",
            "    M    |         \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reward"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ARJXRh1zn-G0",
        "outputId": "fb37c3a9-4ad8-42e1-f807-31c54ce8f2d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "obs, reward, done,_,info = env.step((5, 4))\n",
        "env.render()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fj0RBIQCoAPB",
        "outputId": "2c29c994-8cfd-4196-98b5-a88f6b1744d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         |         \n",
            "         |         \n",
            "         |   H     \n",
            "   H     | HHHHH   \n",
            "         |   H     \n",
            "    MM   |  HHH    \n",
            "         |         \n",
            "    M    |         \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reward"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TrJRnYG6oDpf",
        "outputId": "9823e8f0-ba2e-4dff-a2ca-fed69ed699f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-1"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DQN"
      ],
      "metadata": {
        "id": "IK9qxD9t288H"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HX6Eu4JLoCxf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as T"
      ],
      "metadata": {
        "id": "SSdYfDKwRMOJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gamma = 0.99\n",
        "epsilon = 1.0\n",
        "epsilon_max = 1.0\n",
        "epsilon_min = 0.1\n",
        "epsilon_interval = epsilon_max - epsilon_min\n",
        "batch_size = 16\n",
        "max_steps_per_episode = 60\n",
        "max_episodes = 10000"
      ],
      "metadata": {
        "id": "-v7gRdlQ3ITb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env.observation_space"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jD_N7ySI3iXE",
        "outputId": "5850c846-387c-47bf-a573-f61d4f4744d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Box(0, 255, (8, 8, 1), uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env.action_space"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KAs46CeE3p73",
        "outputId": "722a66fa-e73c-4984-a663-b3fe9388db71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MultiDiscrete([8 8])"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_actions = 64"
      ],
      "metadata": {
        "id": "y3Shy3sL3toi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class QModel(nn.Module):\n",
        "  def __init__(self, num_actions):\n",
        "    super(QModel, self).__init__()\n",
        "    self.dropout = nn.Dropout(p=0.3)\n",
        "    self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding='same')\n",
        "    self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding='same')\n",
        "    self.conv3 = nn.Conv2d(32, 32, kernel_size=3, stride=1)\n",
        "    self.flatten = nn.Flatten()\n",
        "    self.fc1 = nn.Linear(1152, 512)\n",
        "    self.fc2 = nn.Linear(512, num_actions)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = nn.functional.relu(self.conv1(x))\n",
        "    x = nn.functional.relu(self.conv2(x))\n",
        "    x = self.dropout(x)\n",
        "    x = nn.functional.relu(self.conv3(x))\n",
        "    x = self.flatten(x)\n",
        "    x = nn.functional.relu(self.fc1(x))\n",
        "    x = self.dropout(x)\n",
        "    return self.fc2(x)"
      ],
      "metadata": {
        "id": "FrxC01ff3zHN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = QModel(num_actions)"
      ],
      "metadata": {
        "id": "1IqNrPCX5Ua6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_target = QModel(num_actions)"
      ],
      "metadata": {
        "id": "r25E_DsH5dpJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_function = nn.SmoothL1Loss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.00025)"
      ],
      "metadata": {
        "id": "vgSKetaK5gYl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "action_history = []\n",
        "action_mask_history = []\n",
        "state_history = []\n",
        "state_next_history = []\n",
        "rewards_history = []\n",
        "done_history = []"
      ],
      "metadata": {
        "id": "XH941GPa5xiQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "episode_reward_history = []\n",
        "running_reward = 0.\n",
        "episode_count = 0\n",
        "frame_count = 0"
      ],
      "metadata": {
        "id": "dXZhFxRY5_VP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epsilon_random_frames = 50000\n",
        "# Number of frames for exploration\n",
        "epsilon_greedy_frames = 200000.0\n",
        "# Maximum replay length\n",
        "# Note: The Deepmind paper suggests 1000000 however this causes memory issues\n",
        "max_memory_length = 500000\n",
        "# Train the model after 4 actions\n",
        "update_after_actions = 4\n",
        "# How often to update the target network\n",
        "update_target_network = 10000"
      ],
      "metadata": {
        "id": "i1qSeSvE6JDy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_state(obs):\n",
        "  st = torch.from_numpy(obs).squeeze()\n",
        "  st = st.to(torch.int64)\n",
        "  st = torch.nn.functional.one_hot(st, num_classes=3)\n",
        "  st = st.permute(2, 0, 1)\n",
        "  return st.to(torch.float32)"
      ],
      "metadata": {
        "id": "opkmq0qY6bS2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "board, info = env.reset()"
      ],
      "metadata": {
        "id": "2u3tsDnh68Fw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "board.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4R3edFrV7A9j",
        "outputId": "9d328e97-b61e-4b89-eb04-869c29e626da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8, 8, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "st = preprocess_state(board)\n",
        "st.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l4ASGws87CA0",
        "outputId": "f450f59d-d5d9-48b8-d9ac-0fdeb41d71b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 8, 8])"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "st"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KNmJgW_V7HNn",
        "outputId": "377230f1-baca-471c-ab20-bdaae663d01d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "         [1., 1., 1., 1., 1., 1., 1., 1.]],\n",
              "\n",
              "        [[0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0.]],\n",
              "\n",
              "        [[0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0.]]])"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_greedy_epsilon(model, state, mask):\n",
        "  global epsilon\n",
        "\n",
        "  #if frame_count < epsilon_random_frames or np.random.rand(1)[0] < epsilon:\n",
        "  if np.random.rand(1)[0] < epsilon:\n",
        "    action = np.random.choice([ i for i in range(num_actions) if mask[i] == 1 ])\n",
        "  else:\n",
        "    with torch.no_grad():\n",
        "      # add a batch axis\n",
        "      state_tensor = state.unsqueeze(0)\n",
        "      # compute the q-values\n",
        "      q_values = model(state_tensor)\n",
        "      # select the q-values of valid actions\n",
        "      action = torch.argmax(\n",
        "        q_values.squeeze() + torch.from_numpy(mask) * 100., dim=0)\n",
        "\n",
        "  epsilon -= epsilon_interval / epsilon_greedy_frames\n",
        "  epsilon = max(epsilon, epsilon_min)\n",
        "  return action"
      ],
      "metadata": {
        "id": "gbMTCOca7L6J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_greedy_action(model, state, mask):\n",
        "  global epsilon\n",
        "  with torch.no_grad():\n",
        "    state_tensor = state.unsqueeze(0) # batch dimension\n",
        "    q_values = model(state_tensor)\n",
        "    action = torch.argmax(\n",
        "      q_values.squeeze() + torch.from_numpy(mask) * 100.,dim=0)\n",
        "  return action"
      ],
      "metadata": {
        "id": "-M96FnX77z_B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sample_batch(_batch_size):\n",
        "  indices = np.random.choice(range(len(done_history)), size=_batch_size, replace=False)\n",
        "  state_sample = np.array([state_history[i].squeeze(0).numpy() for i in indices])\n",
        "  state_next_sample = np.array([state_next_history[i].squeeze(0).numpy() for i in indices])\n",
        "  rewards_sample = np.array([rewards_history[i] for i in indices], dtype=np.float32)\n",
        "  action_sample = np.array([action_history[i] for i in indices])\n",
        "\n",
        "  # action mask is the mask for the valid actions at the '''next''' state\n",
        "  action_mask_sample = np.array([action_mask_history[i] for i in indices])\n",
        "  done_sample = np.array([float(done_history[i]) for i in indices])\n",
        "  return state_sample, state_next_sample, rewards_sample, action_sample, action_mask_sample, done_sample"
      ],
      "metadata": {
        "id": "5Ddv42HL7_qM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def update_network():\n",
        "  state_sample, state_next_sample, rewards_sample, action_sample, action_mask_sample, done_sample = \\\n",
        "    sample_batch(batch_size)\n",
        "\n",
        "  state_sample = torch.tensor(state_sample, dtype=torch.float32)\n",
        "  state_next_sample = torch.tensor(state_next_sample, dtype=torch.float32)\n",
        "  action_sample = torch.tensor(action_sample, dtype=torch.int64)\n",
        "  action_mask_sample = torch.tensor(action_mask_sample, dtype=torch.int64)\n",
        "  rewards_sample = torch.tensor(rewards_sample, dtype=torch.float32)\n",
        "  done_sample = torch.tensor(done_sample, dtype=torch.float32)\n",
        "\n",
        "  with torch.no_grad():\n",
        "    future_rewards = model_target(state_next_sample)\n",
        "    max_q_values = torch.max (\n",
        "        future_rewards + action_mask_sample * 100.,\n",
        "    dim=1).values.detach() - 100.\n",
        "    target_q_values = rewards_sample + gamma * max_q_values * (1. - done_sample)\n",
        "\n",
        "  q_values = model(state_sample)\n",
        "  q_values_action = q_values.gather(dim=1, index=action_sample.unsqueeze(1)).squeeze(1)\n",
        "  loss = loss_function(q_values_action, target_q_values)\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  optimizer.step()\n"
      ],
      "metadata": {
        "id": "JWDqDT-j8xo_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for _ in range(max_episodes):\n",
        "  state, info = env.reset()\n",
        "  state = preprocess_state(state)\n",
        "  action_mask = info['action_mask'].reshape((-1,))\n",
        "  episode_reward = 0\n",
        "\n",
        "  for timestep in range(1, max_steps_per_episode):\n",
        "    frame_count += 1\n",
        "\n",
        "    action = get_greedy_epsilon(model, state, action_mask)\n",
        "\n",
        "    state_next, reward, done,_, info = env.step((action // 8, action % 8))\n",
        "    state_next = preprocess_state(state_next)\n",
        "    action_mask = info['action_mask'].reshape((-1,))\n",
        "\n",
        "    episode_reward += reward\n",
        "\n",
        "    action_history.append(action)\n",
        "    action_mask_history.append(action_mask)\n",
        "    state_history.append(state)\n",
        "    state_next_history.append(state_next)\n",
        "    rewards_history.append(reward)\n",
        "    done_history.append(done)\n",
        "\n",
        "    state = state_next\n",
        "\n",
        "    if frame_count % update_after_actions == 0 and len(done_history) > batch_size:\n",
        "      update_network()\n",
        "\n",
        "    if frame_count % update_target_network == 0:\n",
        "      model_target.load_state_dict(model.state_dict())\n",
        "\n",
        "    if len(rewards_history) > max_memory_length:\n",
        "      del rewards_history[:1]\n",
        "      del state_history[:1]\n",
        "      del state_next_history[:1]\n",
        "      del action_history[:1]\n",
        "      del action_mask_history[:1]\n",
        "      del done_history[:1]\n",
        "    if done:\n",
        "      break\n",
        "\n",
        "  episode_count +=1\n",
        "  episode_reward_history.append(episode_reward)\n",
        "\n",
        "  if len(episode_reward_history)> 100:\n",
        "    del episode_reward_history[0]\n",
        "\n",
        "  running_reward = np.mean(episode_reward_history)\n",
        "\n",
        "  if episode_count % 10 == 0:\n",
        "    print(episode_count, frame_count, running_reward)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ji7__Lql-IWh",
        "outputId": "fdfdbebd-44f8-4cf7-e8c6-0d2cc92e4788"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10 580 -39.5\n",
            "20 1160 -39.65\n",
            "30 1742 -39.7\n",
            "40 2294 -38.775\n",
            "50 2863 -38.68\n",
            "60 3420 -38.35\n",
            "70 4002 -38.642857142857146\n",
            "80 4570 -38.5625\n",
            "90 5144 -38.56666666666667\n",
            "100 5691 -38.26\n",
            "110 6265 -38.17\n",
            "120 6843 -38.09\n",
            "130 7418 -37.94\n",
            "140 7987 -38.07\n",
            "150 8568 -38.27\n",
            "160 9135 -38.41\n",
            "170 9711 -38.25\n",
            "180 10286 -38.36\n",
            "190 10818 -37.84\n",
            "200 11362 -37.77\n",
            "210 11928 -37.67\n",
            "220 12475 -37.3\n",
            "230 13042 -37.26\n",
            "240 13569 -36.86\n",
            "250 14121 -36.43\n",
            "260 14680 -36.31\n",
            "270 15226 -35.93\n",
            "280 15777 -35.67\n",
            "290 16314 -35.8\n",
            "300 16887 -36.17\n",
            "310 17439 -36.05\n",
            "320 17946 -35.59\n",
            "330 18506 -35.46\n",
            "340 18994 -35.05\n",
            "350 19494 -34.45\n",
            "360 19997 -33.83\n",
            "370 20535 -33.75\n",
            "380 21043 -33.22\n",
            "390 21594 -33.32\n",
            "400 22087 -32.42\n",
            "410 22605 -32.0\n",
            "420 23145 -32.41\n",
            "430 23662 -31.92\n",
            "440 24184 -32.26\n",
            "450 24671 -32.13\n",
            "460 25148 -31.85\n",
            "470 25662 -31.63\n",
            "480 26135 -31.26\n",
            "490 26631 -30.65\n",
            "500 27144 -30.89\n",
            "510 27652 -30.79\n",
            "520 28177 -30.64\n",
            "530 28664 -30.36\n",
            "540 29183 -30.29\n",
            "550 29685 -30.48\n",
            "560 30153 -30.37\n",
            "570 30664 -30.32\n",
            "580 31159 -30.54\n",
            "590 31641 -30.42\n",
            "600 32088 -29.7\n",
            "610 32576 -29.52\n",
            "620 33082 -29.25\n",
            "630 33573 -29.29\n",
            "640 34054 -28.93\n",
            "650 34524 -28.57\n",
            "660 35031 -29.0\n",
            "670 35504 -28.58\n",
            "680 35997 -28.56\n",
            "690 36441 -28.18\n",
            "700 36880 -28.12\n",
            "710 37309 -27.47\n",
            "720 37777 -27.11\n",
            "730 38223 -26.66\n",
            "740 38688 -26.5\n",
            "750 39186 -26.78\n",
            "760 39649 -26.3\n",
            "770 40111 -26.21\n",
            "780 40596 -26.13\n",
            "790 41088 -26.59\n",
            "800 41540 -26.7\n",
            "810 42041 -27.42\n",
            "820 42520 -27.53\n",
            "830 42972 -27.59\n",
            "840 43456 -27.82\n",
            "850 43868 -26.96\n",
            "860 44316 -26.83\n",
            "870 44756 -26.59\n",
            "880 45190 -26.06\n",
            "890 45663 -25.89\n",
            "900 46045 -25.19\n",
            "910 46493 -24.66\n",
            "920 46908 -24.0\n",
            "930 47305 -23.43\n",
            "940 47701 -22.49\n",
            "950 48110 -22.46\n",
            "960 48559 -22.45\n",
            "970 48994 -22.44\n",
            "980 49382 -21.98\n",
            "990 49777 -21.18\n",
            "1000 50233 -21.92\n",
            "1010 50685 -21.96\n",
            "1020 51060 -21.56\n",
            "1030 51507 -22.06\n",
            "1040 51940 -22.43\n",
            "1050 52392 -22.86\n",
            "1060 52791 -22.36\n",
            "1070 53192 -22.0\n",
            "1080 53641 -22.61\n",
            "1090 54109 -23.36\n",
            "1100 54531 -23.02\n",
            "1110 54936 -22.55\n",
            "1120 55347 -22.91\n",
            "1130 55725 -22.22\n",
            "1140 56159 -22.25\n",
            "1150 56598 -22.14\n",
            "1160 56964 -21.81\n",
            "1170 57354 -21.68\n",
            "1180 57770 -21.35\n",
            "1190 58174 -20.69\n",
            "1200 58560 -20.33\n",
            "1210 58986 -20.54\n",
            "1220 59410 -20.67\n",
            "1230 59784 -20.63\n",
            "1240 60132 -19.75\n",
            "1250 60533 -19.35\n",
            "1260 60899 -19.35\n",
            "1270 61299 -19.45\n",
            "1280 61653 -18.83\n",
            "1290 62084 -19.1\n",
            "1300 62471 -19.11\n",
            "1310 62884 -18.98\n",
            "1320 63227 -18.17\n",
            "1330 63592 -18.08\n",
            "1340 63960 -18.28\n",
            "1350 64379 -18.48\n",
            "1360 64785 -18.9\n",
            "1370 65211 -19.16\n",
            "1380 65551 -19.02\n",
            "1390 65957 -18.79\n",
            "1400 66349 -18.84\n",
            "1410 66691 -18.13\n",
            "1420 67054 -18.33\n",
            "1430 67419 -18.33\n",
            "1440 67846 -18.92\n",
            "1450 68233 -18.58\n",
            "1460 68605 -18.22\n",
            "1470 68934 -17.25\n",
            "1480 69282 -17.33\n",
            "1490 69661 -17.04\n",
            "1500 70029 -16.8\n",
            "1510 70451 -17.6\n",
            "1520 70851 -17.97\n",
            "1530 71187 -17.68\n",
            "1540 71518 -16.72\n",
            "1550 71890 -16.57\n",
            "1560 72224 -16.19\n",
            "1570 72588 -16.54\n",
            "1580 72966 -16.84\n",
            "1590 73318 -16.57\n",
            "1600 73681 -16.52\n",
            "1610 74025 -15.74\n",
            "1620 74364 -15.13\n",
            "1630 74737 -15.5\n",
            "1640 75077 -15.59\n",
            "1650 75432 -15.42\n",
            "1660 75774 -15.5\n",
            "1670 76091 -15.03\n",
            "1680 76443 -14.77\n",
            "1690 76750 -14.32\n",
            "1700 77084 -14.03\n",
            "1710 77393 -13.68\n",
            "1720 77725 -13.61\n",
            "1730 78085 -13.48\n",
            "1740 78391 -13.14\n",
            "1750 78775 -13.43\n",
            "1760 79116 -13.42\n",
            "1770 79460 -13.69\n",
            "1780 79816 -13.73\n",
            "1790 80159 -14.09\n",
            "1800 80483 -13.99\n",
            "1810 80829 -14.36\n",
            "1820 81188 -14.63\n",
            "1830 81482 -13.97\n",
            "1840 81787 -13.96\n",
            "1850 82127 -13.52\n",
            "1860 82434 -13.18\n",
            "1870 82751 -12.91\n",
            "1880 83059 -12.43\n",
            "1890 83355 -11.96\n",
            "1900 83668 -11.85\n",
            "1910 83975 -11.46\n",
            "1920 84294 -11.06\n",
            "1930 84632 -11.5\n",
            "1940 84933 -11.46\n",
            "1950 85253 -11.26\n",
            "1960 85585 -11.51\n",
            "1970 85926 -11.75\n",
            "1980 86264 -12.05\n",
            "1990 86613 -12.58\n",
            "2000 86898 -12.3\n",
            "2010 87224 -12.49\n",
            "2020 87541 -12.47\n",
            "2030 87894 -12.62\n",
            "2040 88189 -12.56\n",
            "2050 88525 -12.72\n",
            "2060 88873 -12.88\n",
            "2070 89212 -12.86\n",
            "2080 89519 -12.55\n",
            "2090 89819 -12.06\n",
            "2100 90138 -12.4\n",
            "2110 90434 -12.1\n",
            "2120 90737 -11.96\n",
            "2130 91031 -11.37\n",
            "2140 91337 -11.48\n",
            "2150 91640 -11.15\n",
            "2160 91950 -10.77\n",
            "2170 92222 -10.1\n",
            "2180 92519 -10.0\n",
            "2190 92798 -9.79\n",
            "2200 93087 -9.49\n",
            "2210 93376 -9.42\n",
            "2220 93669 -9.32\n",
            "2230 93984 -9.53\n",
            "2240 94306 -9.69\n",
            "2250 94624 -9.84\n",
            "2260 94899 -9.49\n",
            "2270 95197 -9.75\n",
            "2280 95509 -9.9\n",
            "2290 95819 -10.21\n",
            "2300 96121 -10.34\n",
            "2310 96429 -10.53\n",
            "2320 96698 -10.29\n",
            "2330 96987 -10.03\n",
            "2340 97321 -10.15\n",
            "2350 97611 -9.87\n",
            "2360 97923 -10.24\n",
            "2370 98236 -10.39\n",
            "2380 98486 -9.77\n",
            "2390 98827 -10.08\n",
            "2400 99101 -9.8\n",
            "2410 99380 -9.51\n",
            "2420 99676 -9.78\n",
            "2430 99987 -10.0\n",
            "2440 100237 -9.16\n",
            "2450 100515 -9.04\n",
            "2460 100800 -8.77\n",
            "2470 101101 -8.65\n",
            "2480 101397 -9.11\n",
            "2490 101675 -8.48\n",
            "2500 101985 -8.84\n",
            "2510 102303 -9.23\n",
            "2520 102581 -9.05\n",
            "2530 102854 -8.67\n",
            "2540 103187 -9.5\n",
            "2550 103518 -10.03\n",
            "2560 103818 -10.18\n",
            "2570 104115 -10.14\n",
            "2580 104381 -9.84\n",
            "2590 104678 -10.03\n",
            "2600 104983 -9.98\n",
            "2610 105261 -9.58\n",
            "2620 105538 -9.57\n",
            "2630 105817 -9.63\n",
            "2640 106092 -9.05\n",
            "2650 106376 -8.58\n",
            "2660 106640 -8.22\n",
            "2670 106930 -8.15\n",
            "2680 107228 -8.47\n",
            "2690 107521 -8.43\n",
            "2700 107766 -7.83\n",
            "2710 108028 -7.67\n",
            "2720 108303 -7.65\n",
            "2730 108563 -7.46\n",
            "2740 108833 -7.41\n",
            "2750 109084 -7.08\n",
            "2760 109342 -7.02\n",
            "2770 109642 -7.12\n",
            "2780 109898 -6.7\n",
            "2790 110173 -6.52\n",
            "2800 110448 -6.82\n",
            "2810 110714 -6.86\n",
            "2820 110973 -6.7\n",
            "2830 111243 -6.8\n",
            "2840 111532 -6.99\n",
            "2850 111823 -7.39\n",
            "2860 112056 -7.14\n",
            "2870 112355 -7.13\n",
            "2880 112630 -7.32\n",
            "2890 112916 -7.43\n",
            "2900 113199 -7.51\n",
            "2910 113452 -7.38\n",
            "2920 113713 -7.4\n",
            "2930 113975 -7.32\n",
            "2940 114227 -6.95\n",
            "2950 114480 -6.57\n",
            "2960 114730 -6.74\n",
            "2970 115011 -6.56\n",
            "2980 115280 -6.5\n",
            "2990 115552 -6.36\n",
            "3000 115809 -6.1\n",
            "3010 116076 -6.24\n",
            "3020 116320 -6.07\n",
            "3030 116576 -6.01\n",
            "3040 116825 -5.98\n",
            "3050 117076 -5.96\n",
            "3060 117341 -6.11\n",
            "3070 117602 -5.91\n",
            "3080 117857 -5.77\n",
            "3090 118145 -5.93\n",
            "3100 118402 -5.93\n",
            "3110 118633 -5.57\n",
            "3120 118890 -5.7\n",
            "3130 119147 -5.71\n",
            "3140 119402 -5.77\n",
            "3150 119650 -5.74\n",
            "3160 119902 -5.61\n",
            "3170 120190 -5.88\n",
            "3180 120446 -5.89\n",
            "3190 120730 -5.85\n",
            "3200 120967 -5.65\n",
            "3210 121230 -5.97\n",
            "3220 121513 -6.23\n",
            "3230 121765 -6.18\n",
            "3240 122036 -6.34\n",
            "3250 122252 -6.02\n",
            "3260 122510 -6.08\n",
            "3270 122758 -5.68\n",
            "3280 122996 -5.5\n",
            "3290 123257 -5.27\n",
            "3300 123501 -5.34\n",
            "3310 123769 -5.39\n",
            "3320 123992 -4.79\n",
            "3330 124274 -5.09\n",
            "3340 124516 -4.8\n",
            "3350 124795 -5.43\n",
            "3360 125040 -5.3\n",
            "3370 125264 -5.06\n",
            "3380 125492 -4.96\n",
            "3390 125752 -4.95\n",
            "3400 126004 -5.03\n",
            "3410 126260 -4.91\n",
            "3420 126521 -5.29\n",
            "3430 126771 -4.97\n",
            "3440 127028 -5.12\n",
            "3450 127264 -4.69\n",
            "3460 127480 -4.4\n",
            "3470 127738 -4.74\n",
            "3480 127952 -4.6\n",
            "3490 128189 -4.37\n",
            "3500 128430 -4.26\n",
            "3510 128657 -3.97\n",
            "3520 128896 -3.75\n",
            "3530 129137 -3.66\n",
            "3540 129377 -3.49\n",
            "3550 129619 -3.55\n",
            "3560 129860 -3.8\n",
            "3570 130106 -3.68\n",
            "3580 130358 -4.06\n",
            "3590 130576 -3.87\n",
            "3600 130831 -4.01\n",
            "3610 131070 -4.13\n",
            "3620 131322 -4.26\n",
            "3630 131535 -3.98\n",
            "3640 131774 -3.97\n",
            "3650 132000 -3.81\n",
            "3660 132263 -4.03\n",
            "3670 132507 -4.01\n",
            "3680 132744 -3.86\n",
            "3690 133007 -4.31\n",
            "3700 133240 -4.09\n",
            "3710 133479 -4.09\n",
            "3720 133706 -3.84\n",
            "3730 133943 -4.08\n",
            "3740 134203 -4.29\n",
            "3750 134433 -4.33\n",
            "3760 134652 -3.89\n",
            "3770 134907 -4.0\n",
            "3780 135157 -4.13\n",
            "3790 135363 -3.56\n",
            "3800 135604 -3.64\n",
            "3810 135824 -3.45\n",
            "3820 136055 -3.49\n",
            "3830 136283 -3.4\n",
            "3840 136503 -3.0\n",
            "3850 136727 -2.94\n",
            "3860 136975 -3.23\n",
            "3870 137211 -3.04\n",
            "3880 137435 -2.78\n",
            "3890 137690 -3.27\n",
            "3900 137926 -3.22\n",
            "3910 138156 -3.32\n",
            "3920 138394 -3.39\n",
            "3930 138618 -3.35\n",
            "3940 138863 -3.6\n",
            "3950 139097 -3.7\n",
            "3960 139314 -3.39\n",
            "3970 139534 -3.23\n",
            "3980 139745 -3.1\n",
            "3990 139980 -2.9\n",
            "4000 140224 -2.98\n",
            "4010 140459 -3.03\n",
            "4020 140676 -2.82\n",
            "4030 140899 -2.81\n",
            "4040 141132 -2.69\n",
            "4050 141334 -2.37\n",
            "4060 141551 -2.37\n",
            "4070 141791 -2.57\n",
            "4080 142026 -2.81\n",
            "4090 142277 -2.97\n",
            "4100 142491 -2.67\n",
            "4110 142732 -2.73\n",
            "4120 142948 -2.72\n",
            "4130 143179 -2.8\n",
            "4140 143398 -2.66\n",
            "4150 143621 -2.87\n",
            "4160 143817 -2.66\n",
            "4170 144035 -2.44\n",
            "4180 144254 -2.28\n",
            "4190 144477 -2.0\n",
            "4200 144673 -1.82\n",
            "4210 144897 -1.65\n",
            "4220 145126 -1.78\n",
            "4230 145326 -1.47\n",
            "4240 145541 -1.43\n",
            "4250 145748 -1.27\n",
            "4260 145981 -1.64\n",
            "4270 146193 -1.58\n",
            "4280 146420 -1.66\n",
            "4290 146639 -1.62\n",
            "4300 146862 -1.89\n",
            "4310 147085 -1.88\n",
            "4320 147293 -1.67\n",
            "4330 147531 -2.05\n",
            "4340 147734 -1.93\n",
            "4350 147939 -1.91\n",
            "4360 148117 -1.36\n",
            "4370 148344 -1.51\n",
            "4380 148533 -1.13\n",
            "4390 148747 -1.08\n",
            "4400 148953 -0.91\n",
            "4410 149191 -1.06\n",
            "4420 149442 -1.49\n",
            "4430 149672 -1.41\n",
            "4440 149896 -1.62\n",
            "4450 150096 -1.57\n",
            "4460 150301 -1.84\n",
            "4470 150515 -1.71\n",
            "4480 150733 -2.0\n",
            "4490 150943 -1.96\n",
            "4500 151154 -2.01\n",
            "4510 151372 -1.81\n",
            "4520 151568 -1.26\n",
            "4530 151787 -1.15\n",
            "4540 152008 -1.12\n",
            "4550 152240 -1.44\n",
            "4560 152440 -1.39\n",
            "4570 152664 -1.49\n",
            "4580 152876 -1.43\n",
            "4590 153101 -1.58\n",
            "4600 153319 -1.65\n",
            "4610 153534 -1.62\n",
            "4620 153753 -1.85\n",
            "4630 153938 -1.51\n",
            "4640 154163 -1.55\n",
            "4650 154375 -1.35\n",
            "4660 154586 -1.46\n",
            "4670 154785 -1.21\n",
            "4680 155009 -1.33\n",
            "4690 155187 -0.86\n",
            "4700 155410 -0.91\n",
            "4710 155593 -0.59\n",
            "4720 155781 -0.28\n",
            "4730 155972 -0.34\n",
            "4740 156191 -0.28\n",
            "4750 156381 -0.06\n",
            "4760 156590 -0.04\n",
            "4770 156784 0.01\n",
            "4780 156982 0.27\n",
            "4790 157178 0.09\n",
            "4800 157389 0.21\n",
            "4810 157572 0.21\n",
            "4820 157795 -0.14\n",
            "4830 158001 -0.29\n",
            "4840 158193 -0.02\n",
            "4850 158414 -0.33\n",
            "4860 158609 -0.19\n",
            "4870 158799 -0.15\n",
            "4880 159047 -0.65\n",
            "4890 159246 -0.68\n",
            "4900 159435 -0.46\n",
            "4910 159647 -0.75\n",
            "4920 159843 -0.48\n",
            "4930 160057 -0.56\n",
            "4940 160268 -0.75\n",
            "4950 160465 -0.51\n",
            "4960 160651 -0.42\n",
            "4970 160863 -0.64\n",
            "4980 161050 -0.03\n",
            "4990 161234 0.12\n",
            "5000 161442 -0.07\n",
            "5010 161616 0.31\n",
            "5020 161792 0.51\n",
            "5030 161981 0.76\n",
            "5040 162188 0.8\n",
            "5050 162394 0.71\n",
            "5060 162576 0.75\n",
            "5070 162775 0.88\n",
            "5080 162994 0.56\n",
            "5090 163182 0.52\n",
            "5100 163377 0.65\n",
            "5110 163618 -0.02\n",
            "5120 163802 -0.1\n",
            "5130 164004 -0.23\n",
            "5140 164199 -0.11\n",
            "5150 164413 -0.19\n",
            "5160 164592 -0.16\n",
            "5170 164793 -0.18\n",
            "5180 164989 0.05\n",
            "5190 165182 0.0\n",
            "5200 165389 -0.12\n",
            "5210 165606 0.12\n",
            "5220 165800 0.02\n",
            "5230 166003 0.01\n",
            "5240 166227 -0.28\n",
            "5250 166425 -0.12\n",
            "5260 166609 -0.17\n",
            "5270 166813 -0.2\n",
            "5280 167010 -0.21\n",
            "5290 167207 -0.25\n",
            "5300 167396 -0.07\n",
            "5310 167616 -0.1\n",
            "5320 167806 -0.06\n",
            "5330 168019 -0.16\n",
            "5340 168201 0.26\n",
            "5350 168371 0.54\n",
            "5360 168589 0.2\n",
            "5370 168773 0.4\n",
            "5380 168970 0.4\n",
            "5390 169150 0.57\n",
            "5400 169342 0.54\n",
            "5410 169568 0.48\n",
            "5420 169745 0.61\n",
            "5430 169920 0.99\n",
            "5440 170125 0.76\n",
            "5450 170335 0.36\n",
            "5460 170516 0.73\n",
            "5470 170732 0.41\n",
            "5480 170929 0.41\n",
            "5490 171116 0.34\n",
            "5500 171297 0.45\n",
            "5510 171473 0.95\n",
            "5520 171671 0.74\n",
            "5530 171886 0.34\n",
            "5540 172071 0.54\n",
            "5550 172238 0.97\n",
            "5560 172411 1.05\n",
            "5570 172581 1.51\n",
            "5580 172780 1.49\n",
            "5590 172964 1.52\n",
            "5600 173160 1.37\n",
            "5610 173340 1.33\n",
            "5620 173527 1.44\n",
            "5630 173721 1.65\n",
            "5640 173901 1.7\n",
            "5650 174090 1.48\n",
            "5660 174267 1.44\n",
            "5670 174446 1.35\n",
            "5680 174639 1.41\n",
            "5690 174825 1.39\n",
            "5700 175017 1.43\n",
            "5710 175190 1.5\n",
            "5720 175370 1.57\n",
            "5730 175566 1.55\n",
            "5740 175760 1.41\n",
            "5750 175940 1.5\n",
            "5760 176129 1.38\n",
            "5770 176319 1.27\n",
            "5780 176511 1.28\n",
            "5790 176692 1.33\n",
            "5800 176881 1.36\n",
            "5810 177064 1.26\n",
            "5820 177250 1.2\n",
            "5830 177435 1.31\n",
            "5840 177629 1.31\n",
            "5850 177815 1.25\n",
            "5860 178005 1.24\n",
            "5870 178192 1.27\n",
            "5880 178372 1.39\n",
            "5890 178559 1.33\n",
            "5900 178749 1.32\n",
            "5910 178924 1.4\n",
            "5920 179113 1.37\n",
            "5930 179315 1.2\n",
            "5940 179490 1.39\n",
            "5950 179666 1.49\n",
            "5960 179839 1.66\n",
            "5970 180032 1.6\n",
            "5980 180202 1.7\n",
            "5990 180386 1.73\n",
            "6000 180572 1.77\n",
            "6010 180763 1.61\n",
            "6020 180931 1.82\n",
            "6030 181093 2.22\n",
            "6040 181264 2.26\n",
            "6050 181445 2.21\n",
            "6060 181637 2.02\n",
            "6070 181827 2.05\n",
            "6080 182001 2.01\n",
            "6090 182167 2.19\n",
            "6100 182347 2.25\n",
            "6110 182524 2.39\n",
            "6120 182706 2.25\n",
            "6130 182882 2.11\n",
            "6140 183065 1.99\n",
            "6150 183245 2.0\n",
            "6160 183415 2.22\n",
            "6170 183611 2.16\n",
            "6180 183806 1.95\n",
            "6190 183990 1.77\n",
            "6200 184169 1.78\n",
            "6210 184352 1.72\n",
            "6220 184531 1.75\n",
            "6230 184703 1.79\n",
            "6240 184868 1.97\n",
            "6250 185028 2.17\n",
            "6260 185209 2.06\n",
            "6270 185375 2.36\n",
            "6280 185546 2.6\n",
            "6290 185714 2.76\n",
            "6300 185888 2.81\n",
            "6310 186077 2.75\n",
            "6320 186254 2.77\n",
            "6330 186446 2.57\n",
            "6340 186637 2.31\n",
            "6350 186814 2.14\n",
            "6360 186996 2.13\n",
            "6370 187180 1.95\n",
            "6380 187362 1.84\n",
            "6390 187542 1.72\n",
            "6400 187725 1.63\n",
            "6410 187896 1.81\n",
            "6420 188059 1.95\n",
            "6430 188232 2.14\n",
            "6440 188395 2.42\n",
            "6450 188567 2.47\n",
            "6460 188743 2.53\n",
            "6470 188931 2.49\n",
            "6480 189097 2.65\n",
            "6490 189254 2.88\n",
            "6500 189421 3.04\n",
            "6510 189582 3.14\n",
            "6520 189759 3.0\n",
            "6530 189920 3.12\n",
            "6540 190105 2.9\n",
            "6550 190308 2.59\n",
            "6560 190472 2.71\n",
            "6570 190630 3.01\n",
            "6580 190798 2.99\n",
            "6590 190971 2.83\n",
            "6600 191143 2.78\n",
            "6610 191321 2.61\n",
            "6620 191469 2.9\n",
            "6630 191632 2.88\n",
            "6640 191808 2.97\n",
            "6650 191976 3.32\n",
            "6660 192148 3.24\n",
            "6670 192325 3.05\n",
            "6680 192486 3.12\n",
            "6690 192648 3.23\n",
            "6700 192807 3.36\n",
            "6710 192976 3.45\n",
            "6720 193152 3.17\n",
            "6730 193318 3.14\n",
            "6740 193506 3.02\n",
            "6750 193672 3.04\n",
            "6760 193862 2.86\n",
            "6770 194022 3.03\n",
            "6780 194190 2.96\n",
            "6790 194357 2.91\n",
            "6800 194521 2.86\n",
            "6810 194684 2.92\n",
            "6820 194872 2.8\n",
            "6830 195040 2.78\n",
            "6840 195197 3.09\n",
            "6850 195348 3.24\n",
            "6860 195509 3.53\n",
            "6870 195673 3.49\n",
            "6880 195837 3.53\n",
            "6890 195999 3.58\n",
            "6900 196162 3.59\n",
            "6910 196317 3.67\n",
            "6920 196479 3.93\n",
            "6930 196632 4.08\n",
            "6940 196789 4.08\n",
            "6950 196943 4.05\n",
            "6960 197092 4.17\n",
            "6970 197262 4.11\n",
            "6980 197428 4.09\n",
            "6990 197583 4.16\n",
            "7000 197743 4.19\n",
            "7010 197903 4.14\n",
            "7020 198065 4.14\n",
            "7030 198227 4.05\n",
            "7040 198375 4.14\n",
            "7050 198549 3.94\n",
            "7060 198695 3.97\n",
            "7070 198858 4.04\n",
            "7080 199020 4.08\n",
            "7090 199193 3.9\n",
            "7100 199366 3.77\n",
            "7110 199514 3.89\n",
            "7120 199663 4.02\n",
            "7130 199828 3.99\n",
            "7140 199995 3.8\n",
            "7150 200183 3.66\n",
            "7160 200337 3.58\n",
            "7170 200500 3.58\n",
            "7180 200668 3.52\n",
            "7190 200851 3.42\n",
            "7200 201025 3.41\n",
            "7210 201180 3.34\n",
            "7220 201349 3.14\n",
            "7230 201519 3.09\n",
            "7240 201681 3.14\n",
            "7250 201860 3.23\n",
            "7260 202020 3.17\n",
            "7270 202192 3.08\n",
            "7280 202356 3.12\n",
            "7290 202518 3.33\n",
            "7300 202701 3.24\n",
            "7310 202882 2.98\n",
            "7320 203039 3.1\n",
            "7330 203195 3.24\n",
            "7340 203358 3.23\n",
            "7350 203513 3.47\n",
            "7360 203682 3.38\n",
            "7370 203828 3.64\n",
            "7380 203984 3.72\n",
            "7390 204151 3.67\n",
            "7400 204302 3.99\n",
            "7410 204459 4.23\n",
            "7420 204616 4.23\n",
            "7430 204793 4.02\n",
            "7440 204954 4.04\n",
            "7450 205117 3.96\n",
            "7460 205277 4.05\n",
            "7470 205430 3.98\n",
            "7480 205591 3.93\n",
            "7490 205756 3.95\n",
            "7500 205915 3.87\n",
            "7510 206067 3.92\n",
            "7520 206228 3.88\n",
            "7530 206391 4.02\n",
            "7540 206558 3.96\n",
            "7550 206709 4.08\n",
            "7560 206869 4.08\n",
            "7570 207023 4.07\n",
            "7580 207178 4.13\n",
            "7590 207337 4.19\n",
            "7600 207500 4.15\n",
            "7610 207663 4.04\n",
            "7620 207832 3.96\n",
            "7630 208004 3.87\n",
            "7640 208168 3.9\n",
            "7650 208345 3.64\n",
            "7660 208511 3.58\n",
            "7670 208673 3.5\n",
            "7680 208844 3.34\n",
            "7690 209015 3.22\n",
            "7700 209172 3.28\n",
            "7710 209340 3.23\n",
            "7720 209498 3.34\n",
            "7730 209655 3.49\n",
            "7740 209821 3.47\n",
            "7750 209989 3.56\n",
            "7760 210146 3.65\n",
            "7770 210316 3.57\n",
            "7780 210479 3.65\n",
            "7790 210641 3.74\n",
            "7800 210818 3.54\n",
            "7810 210985 3.55\n",
            "7820 211140 3.58\n",
            "7830 211308 3.47\n",
            "7840 211477 3.44\n",
            "7850 211646 3.43\n",
            "7860 211801 3.45\n",
            "7870 211960 3.56\n",
            "7880 212131 3.48\n",
            "7890 212301 3.4\n",
            "7900 212468 3.5\n",
            "7910 212626 3.59\n",
            "7920 212790 3.5\n",
            "7930 212933 3.75\n",
            "7940 213096 3.81\n",
            "7950 213255 3.91\n",
            "7960 213422 3.79\n",
            "7970 213580 3.8\n",
            "7980 213766 3.65\n",
            "7990 213952 3.49\n",
            "8000 214104 3.64\n",
            "8010 214277 3.49\n",
            "8020 214442 3.48\n",
            "8030 214615 3.18\n",
            "8040 214780 3.16\n",
            "8050 214969 2.86\n",
            "8060 215134 2.88\n",
            "8070 215297 2.83\n",
            "8080 215458 3.08\n",
            "8090 215616 3.36\n",
            "8100 215812 2.92\n",
            "8110 215954 3.23\n",
            "8120 216121 3.21\n",
            "8130 216282 3.33\n",
            "8140 216445 3.35\n",
            "8150 216607 3.62\n",
            "8160 216768 3.66\n",
            "8170 216916 3.81\n",
            "8180 217086 3.72\n",
            "8190 217255 3.61\n",
            "8200 217427 3.85\n",
            "8210 217596 3.58\n",
            "8220 217763 3.58\n",
            "8230 217918 3.64\n",
            "8240 218082 3.63\n",
            "8250 218245 3.62\n",
            "8260 218414 3.54\n",
            "8270 218580 3.36\n",
            "8280 218749 3.37\n",
            "8290 218912 3.43\n",
            "8300 219077 3.5\n",
            "8310 219251 3.45\n",
            "8320 219407 3.56\n",
            "8330 219559 3.59\n",
            "8340 219703 3.79\n",
            "8350 219854 3.91\n",
            "8360 220022 3.92\n",
            "8370 220180 4.0\n",
            "8380 220344 4.05\n",
            "8390 220496 4.16\n",
            "8400 220679 3.98\n",
            "8410 220834 4.17\n",
            "8420 220986 4.21\n",
            "8430 221146 4.13\n",
            "8440 221309 3.94\n",
            "8450 221468 3.86\n",
            "8460 221614 4.08\n",
            "8470 221781 3.99\n",
            "8480 221929 4.15\n",
            "8490 222097 3.99\n",
            "8500 222252 4.27\n",
            "8510 222410 4.24\n",
            "8520 222562 4.24\n",
            "8530 222718 4.28\n",
            "8540 222880 4.29\n",
            "8550 223015 4.53\n",
            "8560 223184 4.3\n",
            "8570 223334 4.47\n",
            "8580 223475 4.54\n",
            "8590 223623 4.74\n",
            "8600 223765 4.87\n",
            "8610 223932 4.78\n",
            "8620 224087 4.75\n",
            "8630 224256 4.62\n",
            "8640 224430 4.5\n",
            "8650 224595 4.2\n",
            "8660 224752 4.32\n",
            "8670 224917 4.17\n",
            "8680 225076 3.99\n",
            "8690 225233 3.9\n",
            "8700 225385 3.8\n",
            "8710 225541 3.91\n",
            "8720 225708 3.79\n",
            "8730 225876 3.8\n",
            "8740 226042 3.88\n",
            "8750 226198 3.97\n",
            "8760 226361 3.91\n",
            "8770 226524 3.93\n",
            "8780 226669 4.07\n",
            "8790 226839 3.94\n",
            "8800 226988 3.97\n",
            "8810 227155 3.86\n",
            "8820 227315 3.93\n",
            "8830 227469 4.07\n",
            "8840 227627 4.15\n",
            "8850 227788 4.1\n",
            "8860 227943 4.18\n",
            "8870 228095 4.29\n",
            "8880 228250 4.19\n",
            "8890 228421 4.18\n",
            "8900 228580 4.08\n",
            "8910 228742 4.13\n",
            "8920 228898 4.17\n",
            "8930 229058 4.11\n",
            "8940 229215 4.12\n",
            "8950 229370 4.18\n",
            "8960 229533 4.1\n",
            "8970 229677 4.18\n",
            "8980 229836 4.14\n",
            "8990 230001 4.2\n",
            "9000 230146 4.34\n",
            "9010 230320 4.22\n",
            "9020 230488 4.1\n",
            "9030 230661 3.97\n",
            "9040 230822 3.93\n",
            "9050 230986 3.84\n",
            "9060 231138 3.95\n",
            "9070 231296 3.81\n",
            "9080 231458 3.78\n",
            "9090 231616 3.85\n",
            "9100 231764 3.82\n",
            "9110 231921 3.99\n",
            "9120 232081 4.07\n",
            "9130 232238 4.23\n",
            "9140 232409 4.13\n",
            "9150 232575 4.11\n",
            "9160 232739 3.99\n",
            "9170 232893 4.03\n",
            "9180 233048 4.1\n",
            "9190 233205 4.11\n",
            "9200 233358 4.06\n",
            "9210 233520 4.01\n",
            "9220 233685 3.96\n",
            "9230 233836 4.02\n",
            "9240 233991 4.18\n",
            "9250 234160 4.15\n",
            "9260 234313 4.26\n",
            "9270 234460 4.33\n",
            "9280 234644 4.04\n",
            "9290 234808 3.97\n",
            "9300 234968 3.9\n",
            "9310 235140 3.8\n",
            "9320 235318 3.67\n",
            "9330 235470 3.66\n",
            "9340 235639 3.52\n",
            "9350 235797 3.63\n",
            "9360 235965 3.48\n",
            "9370 236112 3.48\n",
            "9380 236280 3.64\n",
            "9390 236443 3.65\n",
            "9400 236614 3.54\n",
            "9410 236771 3.69\n",
            "9420 236949 3.69\n",
            "9430 237100 3.7\n",
            "9440 237251 3.88\n",
            "9450 237433 3.64\n",
            "9460 237582 3.83\n",
            "9470 237742 3.7\n",
            "9480 237896 3.84\n",
            "9490 238055 3.88\n",
            "9500 238220 3.94\n",
            "9510 238375 3.96\n",
            "9520 238525 4.24\n",
            "9530 238679 4.21\n",
            "9540 238828 4.23\n",
            "9550 238992 4.41\n",
            "9560 239144 4.38\n",
            "9570 239291 4.51\n",
            "9580 239452 4.44\n",
            "9590 239609 4.46\n",
            "9600 239768 4.52\n",
            "9610 239925 4.5\n",
            "9620 240087 4.38\n",
            "9630 240250 4.29\n",
            "9640 240416 4.12\n",
            "9650 240570 4.22\n",
            "9660 240732 4.12\n",
            "9670 240879 4.12\n",
            "9680 241052 4.0\n",
            "9690 241210 3.99\n",
            "9700 241369 3.99\n",
            "9710 241521 4.04\n",
            "9720 241690 3.97\n",
            "9730 241861 3.89\n",
            "9740 242004 4.12\n",
            "9750 242162 4.08\n",
            "9760 242320 4.12\n",
            "9770 242475 4.04\n",
            "9780 242629 4.23\n",
            "9790 242785 4.25\n",
            "9800 242948 4.21\n",
            "9810 243109 4.12\n",
            "9820 243257 4.33\n",
            "9830 243414 4.47\n",
            "9840 243563 4.41\n",
            "9850 243716 4.46\n",
            "9860 243874 4.46\n",
            "9870 244033 4.42\n",
            "9880 244192 4.37\n",
            "9890 244358 4.27\n",
            "9900 244517 4.31\n",
            "9910 244685 4.24\n",
            "9920 244827 4.3\n",
            "9930 244992 4.22\n",
            "9940 245155 4.08\n",
            "9950 245306 4.1\n",
            "9960 245458 4.16\n",
            "9970 245614 4.19\n",
            "9980 245769 4.23\n",
            "9990 245930 4.28\n",
            "10000 246098 4.19\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time, sys\n",
        "from IPython.display import clear_output\n",
        "board, info = env.reset()\n",
        "state = preprocess_state(board)\n",
        "action_mask = info['action_mask'].reshape((-1,))\n",
        "done = False\n",
        "env.render()\n",
        "while not done:\n",
        "  action = get_greedy_action(model, state, action_mask)\n",
        "  print(\"action: ({}, {})\".format(action // 8, action % 8))\n",
        "  sys.stdout.flush()\n",
        "  time.sleep(1.0)\n",
        "  clear_output(wait=False)\n",
        "  board, reward, done, _,info = env.step((action // 8, action % 8))\n",
        "  state = preprocess_state(board)\n",
        "  action_mask = info['action_mask'].reshape((-1,))\n",
        "  env.render()\n"
      ],
      "metadata": {
        "id": "K5_QSgHxCdQe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "436d3f7f-9771-4551-c1c3-3a83f407813a"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         |         \n",
            "  MHM    |    H    \n",
            " HMH     |  H H    \n",
            " HHHHM   |  HHHH   \n",
            " H H M   |  H H    \n",
            " M HM    |    H    \n",
            "         |         \n",
            "         |         \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UnEgevgckewn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
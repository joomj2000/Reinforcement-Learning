{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joomj2000/Reinforcement-Learning/blob/main/deep_qlearn_reversi.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ZfPRt6dvp-vq",
      "metadata": {
        "id": "ZfPRt6dvp-vq"
      },
      "source": [
        "# Required packages & your custum environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "hB4l0jK3os_M",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hB4l0jK3os_M",
        "outputId": "7b112265-5b29-47da-dc91-c072d71b7a5f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gymnasium\n",
            "  Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.25.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.12.2)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium)\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Installing collected packages: farama-notifications, gymnasium\n",
            "Successfully installed farama-notifications-0.0.4 gymnasium-0.29.1\n"
          ]
        }
      ],
      "source": [
        "!pip install gymnasium"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "t5RLz0TOoy95",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t5RLz0TOoy95",
        "outputId": "f337371f-da85-4984-8a9f-14e8ccbf6a31"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "lEhYL1OKpch_",
      "metadata": {
        "id": "lEhYL1OKpch_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7604943a-7745-4bae-c7a3-dffd99cbf167"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cp: 'drive' and '/content/drive' are the same file\n"
          ]
        }
      ],
      "source": [
        "!cp -r drive /content/drive/MyDrive/SKT_강화학습/gym_examples /content"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "-uu9osLeqjSJ",
      "metadata": {
        "id": "-uu9osLeqjSJ"
      },
      "source": [
        "# Importations & constants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "be000272-b287-41c7-b5ed-1bd352af1a71",
      "metadata": {
        "id": "be000272-b287-41c7-b5ed-1bd352af1a71",
        "tags": []
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as T\n",
        "\n",
        "# Configuration paramaters for the whole setup\n",
        "seed = 42\n",
        "gamma = 0.99  # Discount factor for past rewards\n",
        "epsilon = 1.0  # Epsilon greedy parameter\n",
        "epsilon_min = 0.1  # Minimum epsilon greedy parameter\n",
        "epsilon_max = 1.0  # Maximum epsilon greedy parameter\n",
        "epsilon_interval = epsilon_max - epsilon_min # Rate at which to reduce chance\n",
        "                                             # of random action being taken\n",
        "batch_size = 16  # Size of batch taken from replay buffer\n",
        "max_steps_per_episode = 60\n",
        "max_episodes = 5000\n",
        "\n",
        "num_actions = 64"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "FFVVSMNXp8W9",
      "metadata": {
        "id": "FFVVSMNXp8W9"
      },
      "outputs": [],
      "source": [
        "# Experience replay buffers\n",
        "action_history = []\n",
        "action_mask_history = []\n",
        "state_history = []\n",
        "state_next_history = []\n",
        "rewards_history = []\n",
        "done_history = []\n",
        "episode_reward_history = []\n",
        "\n",
        "# Variables for counting over episodes\n",
        "running_reward = 0\n",
        "episode_count = 0\n",
        "frame_count = 0\n",
        "# Number of frames to take random action and observe output\n",
        "epsilon_random_frames = 1000\n",
        "# Number of frames for exploration\n",
        "epsilon_greedy_frames = 10000.0\n",
        "# Maximum replay length\n",
        "max_memory_length = 500000\n",
        "# Train the model after 4 actions\n",
        "update_after_actions = 4\n",
        "# How often to update the target network\n",
        "update_target_network = 10000"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "yW65jkIvqnKe",
      "metadata": {
        "id": "yW65jkIvqnKe"
      },
      "source": [
        "# Loading gym environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "5bdb4550-1558-4ba7-a910-9faa34652511",
      "metadata": {
        "id": "5bdb4550-1558-4ba7-a910-9faa34652511",
        "tags": []
      },
      "outputs": [],
      "source": [
        "env = gym.make('gym_examples:gym_examples/Reversi-v0', render_mode=\"text\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "68262c3f",
      "metadata": {
        "id": "68262c3f"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "224573a5",
      "metadata": {
        "id": "224573a5"
      },
      "outputs": [],
      "source": [
        "# Function to preprocess the state\n",
        "def preprocess_state(env_observ):\n",
        "    st = torch.from_numpy(env_observ).squeeze()\n",
        "    st = st.to(torch.int64)\n",
        "    st = torch.nn.functional.one_hot(st,num_classes=3)\n",
        "    st = st.permute(2, 0, 1)\n",
        "    return st.to(torch.float32)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "plBCTNfiqt3n",
      "metadata": {
        "id": "plBCTNfiqt3n"
      },
      "source": [
        "# Model definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "f85ef96c-cc48-426a-b2f3-12e002502bc9",
      "metadata": {
        "id": "f85ef96c-cc48-426a-b2f3-12e002502bc9",
        "tags": []
      },
      "outputs": [],
      "source": [
        "class QModel(nn.Module):\n",
        "    def __init__(self, num_actions):\n",
        "        super(QModel, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=0.3)\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=5, stride=1, padding='same')\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=5, stride=1, padding='same')\n",
        "        self.conv3 = nn.Conv2d(32, 32, kernel_size=3, stride=1)\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc1 = nn.Linear(1152, 512)\n",
        "        self.fc2 = nn.Linear(512, num_actions)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = nn.functional.relu(self.conv1(x))\n",
        "        x = nn.functional.relu(self.conv2(x))\n",
        "        x = self.dropout(x)\n",
        "        x = nn.functional.relu(self.conv3(x))\n",
        "        x = self.flatten(x)\n",
        "        x = nn.functional.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        action = self.fc2(x)\n",
        "        return action"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "8a6ec4cd-dd1d-49cf-a50d-fb04628b0f7f",
      "metadata": {
        "id": "8a6ec4cd-dd1d-49cf-a50d-fb04628b0f7f"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# The first model makes the predictions for Q-values which are used to\n",
        "# make a action.\n",
        "model = QModel(num_actions)\n",
        "model.to(device)\n",
        "\n",
        "# Build a target model for the prediction of future rewards.\n",
        "# The weights of a target model get updated every 10000 steps thus when the\n",
        "# loss between the Q-values is calculated the target Q-value is stable.\n",
        "model_target = QModel(num_actions)\n",
        "model_target.to(device)\n",
        "\n",
        "loss_function = nn.SmoothL1Loss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.00025)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ip74_Tt4VW-O",
        "outputId": "98afeee7-d359-4890-ae5c-bf9d7d804058"
      },
      "id": "Ip74_Tt4VW-O",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "MX7sCGp-sRVa",
      "metadata": {
        "id": "MX7sCGp-sRVa"
      },
      "source": [
        "# Policies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "1866419c-5bac-484e-b527-94b07c0087e4",
      "metadata": {
        "id": "1866419c-5bac-484e-b527-94b07c0087e4"
      },
      "outputs": [],
      "source": [
        "# Function to select an action\n",
        "# model: the torch model to compuate action-state value (i.e., q-value)\n",
        "# state: a torch tensor (3 x 8 x 8) of float32, which is output by preprocess_state\n",
        "# mask: a 64-size array (np.array)\n",
        "def get_greedy_epsilon(model, state, mask):\n",
        "    global epsilon\n",
        "\n",
        "    #if frame_count < epsilon_random_frames or np.random.rand(1)[0] < epsilon:\n",
        "    if np.random.rand(1)[0] < epsilon:\n",
        "        action = np.random.choice([ i for i in range(num_actions) if mask[i] == 1 ])\n",
        "    else:\n",
        "        with torch.no_grad():\n",
        "            # add a batch axis\n",
        "            state_tensor = state.unsqueeze(0)\n",
        "            # compute the q-values\n",
        "            q_values = model(state_tensor)\n",
        "            # select the q-values of valid actions\n",
        "            action = torch.argmax(\n",
        "                q_values.to('cpu').squeeze() + torch.from_numpy(mask) * 100., # trick to select a valid action\n",
        "                dim=0)\n",
        "\n",
        "    # decay epsilon\n",
        "    epsilon -= epsilon_interval / epsilon_greedy_frames\n",
        "    epsilon = max(epsilon, epsilon_min)\n",
        "\n",
        "    return action"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "2e67c561-ffd1-4e28-977e-c285430e4d9c",
      "metadata": {
        "id": "2e67c561-ffd1-4e28-977e-c285430e4d9c"
      },
      "outputs": [],
      "source": [
        "def get_greedy_action(model, state, mask):\n",
        "    global epsilon\n",
        "\n",
        "    with torch.no_grad():\n",
        "        state_tensor = state.unsqueeze(0) # batch dimension\n",
        "        q_values = model(state_tensor)\n",
        "\n",
        "        action = torch.argmax(\n",
        "                q_values.to('cpu').squeeze() + torch.from_numpy(mask) * 100., # trick to select a valid action\n",
        "                dim=0)\n",
        "\n",
        "    return action"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "df3cbad6",
      "metadata": {
        "id": "df3cbad6"
      },
      "source": [
        "# Replay Buffer Management"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "94cece0b-2d15-4061-afc5-e94b8cabf7a0",
      "metadata": {
        "id": "94cece0b-2d15-4061-afc5-e94b8cabf7a0"
      },
      "outputs": [],
      "source": [
        "# sample a batch of _batch_size from replay buffers\n",
        "# return numpy.ndarrays\n",
        "def sample_batch(_batch_size):\n",
        "    # Get indices of samples for replay buffers\n",
        "    indices = np.random.choice(range(len(done_history)), size=_batch_size, replace=False)\n",
        "\n",
        "    state_sample = np.array([state_history[i].squeeze(0).numpy() for i in indices])\n",
        "    state_next_sample = np.array([state_next_history[i].squeeze(0).numpy() for i in indices])\n",
        "    rewards_sample = np.array([rewards_history[i] for i in indices], dtype=np.float32)\n",
        "    action_sample = np.array([action_history[i] for i in indices])\n",
        "\n",
        "    # action mask is the mask for the valid actions at the '''next''' state\n",
        "    action_mask_sample = np.array([action_mask_history[i] for i in indices])\n",
        "    done_sample = np.array([float(done_history[i]) for i in indices])\n",
        "\n",
        "    return state_sample, state_next_sample, rewards_sample, action_sample, action_mask_sample, done_sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "zDoOM1rFx7if",
      "metadata": {
        "id": "zDoOM1rFx7if"
      },
      "outputs": [],
      "source": [
        "def append_history(state, state_next, reward, action, action_mask, done):\n",
        "    # Save actions and states in replay buffer\n",
        "    action_history.append(action)\n",
        "    action_mask_history.append(action_mask)\n",
        "    state_history.append(state)\n",
        "    state_next_history.append(state_next)\n",
        "    rewards_history.append(reward)\n",
        "    done_history.append(done)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "3829c226",
      "metadata": {
        "id": "3829c226"
      },
      "outputs": [],
      "source": [
        "# Function to update the Q-network\n",
        "def update_network():\n",
        "    # sample a batch of ...\n",
        "    state_sample, state_next_sample, rewards_sample, action_sample, action_mask_sample, done_sample = \\\n",
        "        sample_batch(batch_size)\n",
        "\n",
        "    # Convert numpy arrays to PyTorch tensors\n",
        "    state_sample = torch.tensor(state_sample, dtype=torch.float32).to(device)\n",
        "    state_next_sample = torch.tensor(state_next_sample, dtype=torch.float32).to(device)\n",
        "    action_sample = torch.tensor(action_sample, dtype=torch.int64).to(device)\n",
        "    action_mask_sample = torch.tensor(action_mask_sample, dtype=torch.int64).to(device)\n",
        "    rewards_sample = torch.tensor(rewards_sample, dtype=torch.float32).to(device)\n",
        "    done_sample = torch.tensor(done_sample, dtype=torch.float32).to(device)\n",
        "\n",
        "    # Compute the target Q-values for the states\n",
        "    with torch.no_grad():\n",
        "        future_rewards = model_target(state_next_sample)\n",
        "        #future_rewards = future_rewards.cpu()\n",
        "\n",
        "        # compute the q-value for the next state and the action maximizing the q-value\n",
        "        # note: the action should be valid (i.e., mask is set to 1)\n",
        "        max_q_values = torch.max(\n",
        "            future_rewards + action_mask_sample * 100., # trick to select a valid action\n",
        "            dim=1).values.detach() - 100.\n",
        "\n",
        "        # compute the target q-value\n",
        "        # if the step was final, max_q_values should not be added\n",
        "        # we assume that the negative return of the opposite player is the return of next step\n",
        "        # that is, G(t) = r(t+1) - g*r(t+2) + g^2*r(t+3) - g^3*r(t+4) + ...\n",
        "        target_q_values = rewards_sample + gamma * max_q_values * (1. - done_sample)\n",
        "\n",
        "    # It's forward propagation! Compute the Q-values for the taken actions\n",
        "    q_values = model(state_sample)\n",
        "    #q_values = q_values.cpu()\n",
        "    q_values_action = q_values.gather(dim=1, index=action_sample.unsqueeze(1)).squeeze(1)\n",
        "\n",
        "    # Compute the loss\n",
        "    loss = loss_function(q_values_action, target_q_values)\n",
        "\n",
        "    # Perform the optimization step\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "90ed4dcc-f24e-4990-879a-c2f6af51a063",
      "metadata": {
        "id": "90ed4dcc-f24e-4990-879a-c2f6af51a063"
      },
      "source": [
        "# Run DQN Tranining"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "Kae3oT2ozMzK",
      "metadata": {
        "id": "Kae3oT2ozMzK"
      },
      "outputs": [],
      "source": [
        "# Experience replay buffers\n",
        "action_history = []\n",
        "action_mask_history = []\n",
        "state_history = []\n",
        "state_next_history = []\n",
        "rewards_history = []\n",
        "done_history = []\n",
        "episode_reward_history = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "343adc5c-37d7-4429-890e-b720a291548c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "343adc5c-37d7-4429-890e-b720a291548c",
        "outputId": "80eafc18-8a37-4901-f302-c3879c1c3b07"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gymnasium/utils/passive_env_checker.py:135: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method was expecting numpy array dtype to be uint8, actual type: int16\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gymnasium/utils/passive_env_checker.py:159: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method is not within the observation space.\u001b[0m\n",
            "  logger.warn(f\"{pre} is not within the observation space.\")\n",
            "/usr/local/lib/python3.10/dist-packages/gymnasium/utils/passive_env_checker.py:135: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method was expecting numpy array dtype to be uint8, actual type: int16\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gymnasium/utils/passive_env_checker.py:159: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method is not within the observation space.\u001b[0m\n",
            "  logger.warn(f\"{pre} is not within the observation space.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode: 10, Frame count: 300, Running reward: -28.6\n",
            "Episode: 20, Frame count: 598, Running reward: -45.1\n",
            "Episode: 30, Frame count: 891, Running reward: -55.06666666666667\n",
            "Episode: 40, Frame count: 1191, Running reward: -55.625\n",
            "Episode: 50, Frame count: 1487, Running reward: -59.12\n",
            "Episode: 60, Frame count: 1787, Running reward: -61.583333333333336\n",
            "Episode: 70, Frame count: 2087, Running reward: -63.357142857142854\n",
            "Episode: 80, Frame count: 2384, Running reward: -66.0375\n",
            "Episode: 90, Frame count: 2681, Running reward: -62.96666666666667\n",
            "Episode: 100, Frame count: 2977, Running reward: -61.9\n",
            "Episode: 110, Frame count: 3273, Running reward: -64.27\n",
            "Episode: 120, Frame count: 3573, Running reward: -64.36\n",
            "Episode: 130, Frame count: 3868, Running reward: -64.41\n",
            "Episode: 140, Frame count: 4166, Running reward: -65.76\n",
            "Episode: 150, Frame count: 4464, Running reward: -65.9\n",
            "Episode: 160, Frame count: 4760, Running reward: -63.37\n",
            "Episode: 170, Frame count: 5059, Running reward: -60.02\n",
            "Episode: 180, Frame count: 5356, Running reward: -57.87\n",
            "Episode: 190, Frame count: 5654, Running reward: -57.08\n",
            "Episode: 200, Frame count: 5950, Running reward: -55.94\n",
            "Episode: 210, Frame count: 6247, Running reward: -55.68\n",
            "Episode: 220, Frame count: 6544, Running reward: -52.2\n",
            "Episode: 230, Frame count: 6843, Running reward: -51.26\n",
            "Episode: 240, Frame count: 7142, Running reward: -50.64\n",
            "Episode: 250, Frame count: 7442, Running reward: -49.46\n",
            "Episode: 260, Frame count: 7742, Running reward: -46.35\n",
            "Episode: 270, Frame count: 8036, Running reward: -47.53\n",
            "Episode: 280, Frame count: 8336, Running reward: -46.15\n",
            "Episode: 290, Frame count: 8633, Running reward: -43.81\n",
            "Episode: 300, Frame count: 8932, Running reward: -43.59\n",
            "Episode: 310, Frame count: 9229, Running reward: -45.91\n",
            "Episode: 320, Frame count: 9524, Running reward: -47.11\n",
            "Episode: 330, Frame count: 9824, Running reward: -43.39\n",
            "Episode: 340, Frame count: 10121, Running reward: -39.58\n",
            "Episode: 350, Frame count: 10420, Running reward: -38.37\n",
            "Episode: 360, Frame count: 10716, Running reward: -40.52\n",
            "Episode: 370, Frame count: 11009, Running reward: -39.48\n",
            "Episode: 380, Frame count: 11307, Running reward: -39.38\n",
            "Episode: 390, Frame count: 11593, Running reward: -44.83\n",
            "Episode: 400, Frame count: 11889, Running reward: -44.96\n",
            "Episode: 410, Frame count: 12188, Running reward: -41.57\n",
            "Episode: 420, Frame count: 12487, Running reward: -41.46\n",
            "Episode: 430, Frame count: 12787, Running reward: -43.7\n",
            "Episode: 440, Frame count: 13086, Running reward: -48.26\n",
            "Episode: 450, Frame count: 13383, Running reward: -45.82\n",
            "Episode: 460, Frame count: 13681, Running reward: -46.77\n",
            "Episode: 470, Frame count: 13968, Running reward: -45.44\n",
            "Episode: 480, Frame count: 14266, Running reward: -43.26\n",
            "Episode: 490, Frame count: 14566, Running reward: -40.92\n",
            "Episode: 500, Frame count: 14862, Running reward: -38.32\n",
            "Episode: 510, Frame count: 15157, Running reward: -38.39\n",
            "Episode: 520, Frame count: 15457, Running reward: -40.86\n",
            "Episode: 530, Frame count: 15755, Running reward: -38.5\n",
            "Episode: 540, Frame count: 16054, Running reward: -32.93\n",
            "Episode: 550, Frame count: 16349, Running reward: -35.21\n",
            "Episode: 560, Frame count: 16648, Running reward: -32.05\n",
            "Episode: 570, Frame count: 16939, Running reward: -30.79\n",
            "Episode: 580, Frame count: 17232, Running reward: -32.13\n",
            "Episode: 590, Frame count: 17529, Running reward: -32.3\n",
            "Episode: 600, Frame count: 17823, Running reward: -32.44\n",
            "Episode: 610, Frame count: 18122, Running reward: -32.52\n",
            "Episode: 620, Frame count: 18416, Running reward: -27.63\n",
            "Episode: 630, Frame count: 18716, Running reward: -27.6\n",
            "Episode: 640, Frame count: 19016, Running reward: -29.66\n",
            "Episode: 650, Frame count: 19312, Running reward: -30.64\n",
            "Episode: 660, Frame count: 19610, Running reward: -33.88\n",
            "Episode: 670, Frame count: 19909, Running reward: -34.99\n",
            "Episode: 680, Frame count: 20205, Running reward: -33.28\n",
            "Episode: 690, Frame count: 20501, Running reward: -28.69\n",
            "Episode: 700, Frame count: 20800, Running reward: -29.81\n",
            "Episode: 710, Frame count: 21100, Running reward: -30.71\n",
            "Episode: 720, Frame count: 21399, Running reward: -31.99\n",
            "Episode: 730, Frame count: 21692, Running reward: -32.79\n",
            "Episode: 740, Frame count: 21992, Running reward: -32.8\n",
            "Episode: 750, Frame count: 22289, Running reward: -28.59\n",
            "Episode: 760, Frame count: 22586, Running reward: -26.29\n",
            "Episode: 770, Frame count: 22881, Running reward: -26.42\n",
            "Episode: 780, Frame count: 23178, Running reward: -29.09\n",
            "Episode: 790, Frame count: 23474, Running reward: -31.19\n",
            "Episode: 800, Frame count: 23772, Running reward: -31.31\n",
            "Episode: 810, Frame count: 24051, Running reward: -28.91\n",
            "Episode: 820, Frame count: 24349, Running reward: -30.04\n",
            "Episode: 830, Frame count: 24646, Running reward: -28.85\n",
            "Episode: 840, Frame count: 24941, Running reward: -28.68\n",
            "Episode: 850, Frame count: 25240, Running reward: -28.62\n",
            "Episode: 860, Frame count: 25535, Running reward: -27.63\n",
            "Episode: 870, Frame count: 25832, Running reward: -25.02\n",
            "Episode: 880, Frame count: 26121, Running reward: -23.86\n",
            "Episode: 890, Frame count: 26420, Running reward: -26.08\n",
            "Episode: 900, Frame count: 26718, Running reward: -23.39\n",
            "Episode: 910, Frame count: 27013, Running reward: -27.89\n",
            "Episode: 920, Frame count: 27289, Running reward: -28.83\n",
            "Episode: 930, Frame count: 27588, Running reward: -29.07\n",
            "Episode: 940, Frame count: 27887, Running reward: -28.33\n",
            "Episode: 950, Frame count: 28187, Running reward: -29.51\n",
            "Episode: 960, Frame count: 28464, Running reward: -29.35\n",
            "Episode: 970, Frame count: 28739, Running reward: -33.11\n",
            "Episode: 980, Frame count: 29038, Running reward: -34.05\n",
            "Episode: 990, Frame count: 29335, Running reward: -35.11\n",
            "Episode: 1000, Frame count: 29632, Running reward: -38.85\n",
            "Episode: 1010, Frame count: 29902, Running reward: -32.48\n",
            "Episode: 1020, Frame count: 30193, Running reward: -28.01\n",
            "Episode: 1030, Frame count: 30490, Running reward: -26.69\n",
            "Episode: 1040, Frame count: 30766, Running reward: -28.02\n",
            "Episode: 1050, Frame count: 31062, Running reward: -26.79\n",
            "Episode: 1060, Frame count: 31338, Running reward: -28.04\n",
            "Episode: 1070, Frame count: 31636, Running reward: -26.68\n",
            "Episode: 1080, Frame count: 31913, Running reward: -27.82\n",
            "Episode: 1090, Frame count: 32212, Running reward: -24.12\n",
            "Episode: 1100, Frame count: 32508, Running reward: -25.14\n",
            "Episode: 1110, Frame count: 32792, Running reward: -27.48\n",
            "Episode: 1120, Frame count: 33085, Running reward: -27.78\n",
            "Episode: 1130, Frame count: 33384, Running reward: -26.8\n",
            "Episode: 1140, Frame count: 33654, Running reward: -24.02\n",
            "Episode: 1150, Frame count: 33954, Running reward: -26.54\n",
            "Episode: 1160, Frame count: 34249, Running reward: -25.34\n",
            "Episode: 1170, Frame count: 34547, Running reward: -24.04\n",
            "Episode: 1180, Frame count: 34845, Running reward: -19.54\n",
            "Episode: 1190, Frame count: 35120, Running reward: -18.82\n",
            "Episode: 1200, Frame count: 35418, Running reward: -17.69\n",
            "Episode: 1210, Frame count: 35712, Running reward: -14.85\n",
            "Episode: 1220, Frame count: 35997, Running reward: -18.81\n",
            "Episode: 1230, Frame count: 36297, Running reward: -17.45\n",
            "Episode: 1240, Frame count: 36593, Running reward: -16.4\n",
            "Episode: 1250, Frame count: 36882, Running reward: -14.91\n",
            "Episode: 1260, Frame count: 37181, Running reward: -15.76\n",
            "Episode: 1270, Frame count: 37479, Running reward: -14.68\n",
            "Episode: 1280, Frame count: 37778, Running reward: -13.67\n",
            "Episode: 1290, Frame count: 38053, Running reward: -15.92\n",
            "Episode: 1300, Frame count: 38350, Running reward: -13.49\n",
            "Episode: 1310, Frame count: 38631, Running reward: -17.12\n",
            "Episode: 1320, Frame count: 38909, Running reward: -17.49\n",
            "Episode: 1330, Frame count: 39206, Running reward: -21.06\n",
            "Episode: 1340, Frame count: 39487, Running reward: -21.19\n",
            "Episode: 1350, Frame count: 39785, Running reward: -21.27\n",
            "Episode: 1360, Frame count: 40081, Running reward: -21.16\n",
            "Episode: 1370, Frame count: 40379, Running reward: -24.48\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-9cf999bf4baa>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;31m# Take the selected action\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mstate_next\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mstate_next\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_next\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0maction_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'action_mask'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gymnasium/wrappers/order_enforcing.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_reset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mResetNeeded\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cannot call env.step() before calling env.reset()\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gymnasium/wrappers/env_checker.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0menv_step_passive_checker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/gym_examples/envs/reversi_random.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 281\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0maction\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpossible_moves\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAGENT_PLAYER\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m         \u001b[0;31m# -------------- fill here --------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAssertionError\u001b[0m: "
          ]
        }
      ],
      "source": [
        "for _ in range(max_episodes):\n",
        "    state, info = env.reset()\n",
        "    state = preprocess_state(state)\n",
        "    action_mask = info['action_mask'].reshape((-1,))\n",
        "    episode_reward = 0\n",
        "\n",
        "    for timestep in range(1, max_steps_per_episode):\n",
        "        frame_count += 1\n",
        "\n",
        "        # Select an action\n",
        "        #state_cuda = state.to(device)\n",
        "        action = get_greedy_epsilon(model,\n",
        "                      state.to(device),\n",
        "                      action_mask)\n",
        "        if action < 0:\n",
        "            print(action_mask)\n",
        "\n",
        "        # Take the selected action\n",
        "        state_next, reward, done, _, info = env.step((action // 8, action % 8))\n",
        "        state_next = preprocess_state(state_next)\n",
        "        action_mask = info['action_mask'].reshape((-1,))\n",
        "\n",
        "        episode_reward += reward\n",
        "\n",
        "        # Store the transition in the replay buffer\n",
        "        append_history(state, state_next, reward, action, action_mask, done)\n",
        "\n",
        "        state = state_next\n",
        "\n",
        "        # Update every fourth frame and once batch size is over 32\n",
        "        if frame_count % update_after_actions == 0 and len(done_history) > batch_size:\n",
        "            update_network()\n",
        "\n",
        "        if frame_count % update_target_network == 0:\n",
        "            model_target.load_state_dict(model.state_dict())\n",
        "\n",
        "        # Limit the state and reward history\n",
        "        if len(rewards_history) > max_memory_length:\n",
        "            del rewards_history[:1]\n",
        "            del state_history[:1]\n",
        "            del state_next_history[:1]\n",
        "            del action_history[:1]\n",
        "            del action_mask_history[:1]\n",
        "            del done_history[:1]\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    episode_count += 1\n",
        "    episode_reward_history.append(episode_reward)\n",
        "\n",
        "    # Update running reward to check condition for solving\n",
        "    if len(episode_reward_history) > 100:\n",
        "        del episode_reward_history[:1]\n",
        "    running_reward = np.mean(episode_reward_history)\n",
        "\n",
        "    if episode_count % 10 == 0:\n",
        "        print(f\"Episode: {episode_count}, Frame count: {frame_count}, Running reward: {running_reward}\")\n",
        "\n",
        "    if episode_count % 5000 == 0:\n",
        "        torch.save(model, 'model.{}'.format(episode_count))\n",
        "    #if running_reward > 20:\n",
        "    #    print(f\"Solved at episode {episode_count}!\")\n",
        "    #    break\n",
        "\n",
        "\n",
        "torch.save(model, 'model.final')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env.render()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ROYwJvbZamcK",
        "outputId": "b2532472-0a41-41d5-eec0-7dad0bb18b8a"
      },
      "id": "ROYwJvbZamcK",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current state of the board:\n",
            "    0 1 2 3 4 5 6 7\n",
            "-------------------\n",
            "0 | . 1 1 1 1 1 1 1\n",
            "1 | 1 . 2 2 2 2 1 1\n",
            "2 | 2 2 2 2 2 2 1 1\n",
            "3 | 2 2 1 2 2 1 2 2\n",
            "4 | 2 1 2 1 1 2 2 2\n",
            "5 | 2 2 1 1 2 2 1 2\n",
            "6 | 2 2 2 2 2 2 2 2\n",
            "7 | 2 2 2 2 2 2 2 2\n",
            "<class 'str'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "4aad31b0-c081-4092-9414-484c98b70370",
      "metadata": {
        "id": "4aad31b0-c081-4092-9414-484c98b70370"
      },
      "outputs": [],
      "source": [
        "torch.save(model.cpu().state_dict(), 'model')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JgYA2pHAoeI2",
        "outputId": "b6070de4-ab4a-4a41-ea6c-f69eaad1afe6"
      },
      "id": "JgYA2pHAoeI2",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "drive  gym_examples  model  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp model drive/MyDrive/Test/SKT"
      ],
      "metadata": {
        "id": "k112vUJmoolQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6c40224-6e34-4a39-8b6f-0cb43b25e45b"
      },
      "id": "k112vUJmoolQ",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cp: cannot create regular file 'drive/MyDrive/Test/SKT': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4984b880-e427-48cb-bf91-13a91d6529f5",
      "metadata": {
        "id": "4984b880-e427-48cb-bf91-13a91d6529f5"
      },
      "source": [
        "# Evaluation (Agent vs. Gym's random play)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "5c198b4e-b0e4-4fd4-821d-be602c2dbc5a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5c198b4e-b0e4-4fd4-821d-be602c2dbc5a",
        "outputId": "d90d6265-a7e7-42fd-b144-e83aea47e3d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current state of the board:\n",
            "    0 1 2 3 4 5 6 7\n",
            "-------------------\n",
            "0 | 1 1 1 1 1 1 1 2\n",
            "1 | 1 1 1 1 1 1 2 1\n",
            "2 | 1 1 1 1 2 2 1 1\n",
            "3 | 1 1 2 1 2 2 2 1\n",
            "4 | 1 1 2 2 1 1 2 1\n",
            "5 | 1 1 2 1 1 1 1 1\n",
            "6 | 1 1 1 2 2 1 1 1\n",
            "7 | 2 1 1 1 1 1 1 1\n",
            "Game over! - Winner 1\n",
            "<class 'str'>\n"
          ]
        }
      ],
      "source": [
        "import time, sys\n",
        "from IPython.display import clear_output\n",
        "\n",
        "board, info = env.reset()\n",
        "state = preprocess_state(board)\n",
        "action_mask = info['action_mask'].reshape((-1,))\n",
        "done = False\n",
        "env.render()\n",
        "\n",
        "while not done:\n",
        "    action = get_greedy_action(model, state.to(device), action_mask)\n",
        "    print(\"action: ({}, {})\".format(action // 8, action % 8))\n",
        "    sys.stdout.flush()\n",
        "\n",
        "    time.sleep(1.0)\n",
        "    clear_output(wait=False)\n",
        "    board, reward, done, _, info = env.step((action // 8, action % 8))\n",
        "    state = preprocess_state(board)\n",
        "    action_mask = info['action_mask'].reshape((-1,))\n",
        "    env.render()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation (Agent vs. Human)"
      ],
      "metadata": {
        "id": "cgKRLo18cnhe"
      },
      "id": "cgKRLo18cnhe"
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "06aaaf66-05b5-4f6a-a004-5016908f8df4",
      "metadata": {
        "id": "06aaaf66-05b5-4f6a-a004-5016908f8df4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f512d457-34af-4ee4-bf4b-e03c3774c1d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current state of the board:\n",
            "    0 1 2 3 4 5 6 7\n",
            "-------------------\n",
            "0 | 2 1 1 1 1 1 1 1\n",
            "1 | 1 1 1 1 1 2 1 1\n",
            "2 | 1 1 1 1 1 1 1 1\n",
            "3 | 1 1 1 1 1 2 2 1\n",
            "4 | 1 1 1 1 2 1 2 1\n",
            "5 | 1 1 2 2 2 2 2 2\n",
            "6 | 1 2 1 2 1 1 2 2\n",
            "7 | 1 1 1 1 1 1 1 2\n",
            "Game over! - Winner 1\n",
            "<class 'str'>\n"
          ]
        }
      ],
      "source": [
        "board, info = env.reset()\n",
        "state = preprocess_state(board)\n",
        "action_mask = info['action_mask'].reshape((-1,))\n",
        "done = False\n",
        "env.render()\n",
        "\n",
        "while not done:\n",
        "    action = get_greedy_action(model, state.to(device), action_mask)\n",
        "    print(\"action: ({}, {})\".format(action // 8, action % 8))\n",
        "    sys.stdout.flush()\n",
        "\n",
        "    time.sleep(1.0)\n",
        "    clear_output(wait=False)\n",
        "    board, reward, done, _, info = env.step((action // 8, action % 8))\n",
        "    state = preprocess_state(board)\n",
        "    action_mask = info['action_mask'].reshape((-1,))\n",
        "    env.render()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "62e0_QAvSYda"
      },
      "id": "62e0_QAvSYda",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
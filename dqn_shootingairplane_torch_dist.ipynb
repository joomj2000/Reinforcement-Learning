{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joomj2000/Reinforcement-Learning/blob/main/dqn_shootingairplane_torch_dist.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R8N-jt71iNWH",
        "outputId": "19fd3858-c190-43ae-8ae1-2f0deea249a1"
      },
      "id": "R8N-jt71iNWH",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r /content/drive/MyDrive/SKT_강화학습/gym_examples /content"
      ],
      "metadata": {
        "id": "vDxRu4Y0iRgl"
      },
      "id": "vDxRu4Y0iRgl",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "250b088e",
      "metadata": {
        "id": "250b088e"
      },
      "source": [
        "# DQN으로 Shooring Airplane Game 강화학습\n",
        "\n",
        "먼저 여러가지 설정 변수 정의"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "be000272-b287-41c7-b5ed-1bd352af1a71",
      "metadata": {
        "tags": [],
        "id": "be000272-b287-41c7-b5ed-1bd352af1a71",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c20f2572-7fe6-4a34-c83c-aa33d55b3e1c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as T\n",
        "\n",
        "# Configuration paramaters for the whole setup\n",
        "seed = 42\n",
        "gamma = 0.99  # Discount factor for past rewards\n",
        "epsilon = 1.0  # Epsilon greedy parameter\n",
        "epsilon_min = 0.1  # Minimum epsilon greedy parameter\n",
        "epsilon_max = 1.0  # Maximum epsilon greedy parameter\n",
        "epsilon_interval = epsilon_max - epsilon_min # Rate at which to reduce chance\n",
        "                                             # of random action being taken\n",
        "batch_size = 16  # Size of batch taken from replay buffer\n",
        "max_steps_per_episode = 60\n",
        "max_episodes = 5000"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "82a3fe92",
      "metadata": {
        "id": "82a3fe92"
      },
      "source": [
        "### 게임 환경 설정\n",
        "\n",
        "상태(state) 정의\n",
        "- 보드판의 모양: (8 * 8) 행렬 * 3 채널\n",
        "- 채널 0: unseen\n",
        "- 채널 1: hit\n",
        "- 채녈 2: miss\n",
        "\n",
        "액션 정의\n",
        "- 돌의 가능한 위치 (8 * 8 = 64)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gymnasium"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6qKuu_suzAVs",
        "outputId": "f18cfc00-4d06-4e8e-c5a4-b87f85c14889"
      },
      "id": "6qKuu_suzAVs",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gymnasium\n",
            "  Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.25.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.12.2)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium)\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Installing collected packages: farama-notifications, gymnasium\n",
            "Successfully installed farama-notifications-0.0.4 gymnasium-0.29.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "5bdb4550-1558-4ba7-a910-9faa34652511",
      "metadata": {
        "tags": [],
        "id": "5bdb4550-1558-4ba7-a910-9faa34652511"
      },
      "outputs": [],
      "source": [
        "env = gym.make('gym_examples:gym_examples/ShootingAirplane-v0', render_mode=\"text\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bfab303e",
      "metadata": {
        "id": "bfab303e"
      },
      "source": [
        "env에서 정의한 action_space, observation_space 의 모양 확인\n",
        "- action_space: 3개의 값의 튜플 (벡터)\n",
        "- observation_space: HWC 형태의 이미지 (마지막 축이 단일 값인 15 * 15 * 1 텐서) -> pytorch를 사용할 경우 적절히 1 * 15 * 15 텐서로 수정필요"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "d3ba8e44",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d3ba8e44",
        "outputId": "c76876dc-e5d5-4517-d098-734a592cb6b5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2,)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "env.action_space.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "d4c3c91f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d4c3c91f",
        "outputId": "738accfd-4333-4930-c279-0d2daaf30595"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8, 8, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "env.observation_space.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5477cab4",
      "metadata": {
        "id": "5477cab4"
      },
      "source": [
        "### 네트워크 정의하기\n",
        "\n",
        "참고: Conv2d 파라미터\n",
        "* in_channels (int) – Number of channels in the input image\n",
        "* out_channels (int) – Number of channels produced by the convolution\n",
        "* kernel_size (int or tuple) – Size of the convolving kernel\n",
        "* stride (int or tuple, optional) – Stride of the convolution. Default: 1\n",
        "* padding (int, tuple or str, optional) – Padding added to all four sides of the input. Default: 0\n",
        "* padding_mode (str, optional) – 'zeros', 'reflect', 'replicate' or 'circular'. Default: 'zeros'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "f85ef96c-cc48-426a-b2f3-12e002502bc9",
      "metadata": {
        "tags": [],
        "id": "f85ef96c-cc48-426a-b2f3-12e002502bc9"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "num_actions = 64\n",
        "\n",
        "class QModel(nn.Module):\n",
        "    def __init__(self, num_actions):\n",
        "        super(QModel, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=0.3)\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding='same')\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding='same')\n",
        "        self.conv3 = nn.Conv2d(32, 32, kernel_size=3, stride=1)\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc1 = nn.Linear(1152, 512)\n",
        "        self.fc2 = nn.Linear(512, num_actions)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = nn.functional.relu(self.conv1(x))\n",
        "        x = nn.functional.relu(self.conv2(x))\n",
        "        x = self.dropout(x)\n",
        "        x = nn.functional.relu(self.conv3(x))\n",
        "        x = self.flatten(x)\n",
        "        x = nn.functional.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        action = self.fc2(x)\n",
        "        return action"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "21a23e4c",
      "metadata": {
        "id": "21a23e4c"
      },
      "source": [
        "### 모델 빌딩 & 로스 및 최적화 계산기 만들기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "8a6ec4cd-dd1d-49cf-a50d-fb04628b0f7f",
      "metadata": {
        "id": "8a6ec4cd-dd1d-49cf-a50d-fb04628b0f7f"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# The first model makes the predictions for Q-values which are used to\n",
        "# make a action.\n",
        "model = QModel(num_actions)\n",
        "model.to(device)\n",
        "\n",
        "# Build a target model for the prediction of future rewards.\n",
        "# The weights of a target model get updated every 10000 steps thus when the\n",
        "# loss between the Q-values is calculated the target Q-value is stable.\n",
        "model_target = QModel(num_actions)\n",
        "model_target.to(device)\n",
        "\n",
        "loss_function = nn.SmoothL1Loss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.00025)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "38d7c152",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "38d7c152",
        "outputId": "38fdf7f0-32f2-4d4a-bf6d-b222e16c83c9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a859f550",
      "metadata": {
        "id": "a859f550"
      },
      "source": [
        "### Replay Buffer 정의"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "8eed8ce4-5a53-457f-bc40-fda577b2ec29",
      "metadata": {
        "id": "8eed8ce4-5a53-457f-bc40-fda577b2ec29"
      },
      "outputs": [],
      "source": [
        "# Experience replay buffers\n",
        "action_history = []\n",
        "action_mask_history = []\n",
        "state_history = []\n",
        "state_next_history = []\n",
        "rewards_history = []\n",
        "done_history = []\n",
        "episode_reward_history = []\n",
        "running_reward = 0\n",
        "episode_count = 0\n",
        "frame_count = 0\n",
        "\n",
        "# Number of frames to take random action and observe output\n",
        "epsilon_random_frames = 50000\n",
        "# Number of frames for exploration\n",
        "epsilon_greedy_frames = 200000.0\n",
        "# Maximum replay length\n",
        "# Note: The Deepmind paper suggests 1000000 however this causes memory issues\n",
        "max_memory_length = 500000\n",
        "# Train the model after 4 actions\n",
        "update_after_actions = 4\n",
        "# How often to update the target network\n",
        "update_target_network = 10000"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eda734a1",
      "metadata": {
        "id": "eda734a1"
      },
      "source": [
        "### 전처리\n",
        "\n",
        "- env가 리턴하는 observation은 일단 np.array이니 torch.tensor로 캐스팅\n",
        "- env가 리턴하는 상태가 (8, 8, 1)의 HWC 이미지 텐서이므로 이를 (3, 15, 15)의 CHW 이미지로 변환\n",
        "- One-hot 인코딩도 필요"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "abf51fd4",
      "metadata": {
        "id": "abf51fd4"
      },
      "outputs": [],
      "source": [
        "# Function to preprocess the state\n",
        "# note that player 1 = env player, player 2 = agent\n",
        "def preprocess_state(env_observ):\n",
        "    st = torch.from_numpy(env_observ).squeeze()\n",
        "    st = st.to(torch.int64)\n",
        "    st = torch.nn.functional.one_hot(st,num_classes=3)\n",
        "    st = st.permute(2, 0, 1)\n",
        "    return st.to(torch.float32)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "61693680",
      "metadata": {
        "id": "61693680"
      },
      "source": [
        "### Epsilon-greedy 액션 선택 함수\n",
        "\n",
        "학습시 에피소드 생성하면서 사용 (주의: 입력은 batch axis 없음)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "f3f9bd3c",
      "metadata": {
        "id": "f3f9bd3c"
      },
      "outputs": [],
      "source": [
        "# Function to select an action\n",
        "# model: the torch model to compuate action-state value (i.e., q-value)\n",
        "# state: a torch tensor (3 x 8 x 8) of float32, which is output by preprocess_state\n",
        "# mask: a 64-size array (np.array)\n",
        "def get_greedy_epsilon(model, state, mask):\n",
        "    global epsilon\n",
        "\n",
        "    #if frame_count < epsilon_random_frames or np.random.rand(1)[0] < epsilon:\n",
        "    if np.random.rand(1)[0] < epsilon:\n",
        "        action = np.random.choice([ i for i in range(num_actions) if mask[i] == 1 ])\n",
        "    else:\n",
        "        with torch.no_grad():\n",
        "            # add a batch axis\n",
        "            state_tensor = state.unsqueeze(0)\n",
        "            # compute the q-values\n",
        "            q_values = model(state_tensor)\n",
        "            # select the q-values of valid actions\n",
        "            action = torch.argmax(\n",
        "                q_values.to('cpu').squeeze() + torch.from_numpy(mask) * 100., # trick to select a valid action\n",
        "                dim=0)\n",
        "\n",
        "\n",
        "            #valid_q = [ (i, q_values[0][i]) for i in range(64) if mask[i] == 1 ]\n",
        "            # the action of maximum q-value\n",
        "            #action, _ = max(valid_q, key=lambda e: e[1])\n",
        "\n",
        "    # decay epsilon\n",
        "    epsilon -= epsilon_interval / epsilon_greedy_frames\n",
        "    epsilon = max(epsilon, epsilon_min)\n",
        "\n",
        "    return action"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ea6728b0",
      "metadata": {
        "id": "ea6728b0"
      },
      "source": [
        "### Greedy 액션 선택 함수\n",
        "\n",
        "나중에 evaluation 시 사용"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "8025c8a5",
      "metadata": {
        "id": "8025c8a5"
      },
      "outputs": [],
      "source": [
        "def get_greedy_action(model, state, mask):\n",
        "    global epsilon\n",
        "\n",
        "    with torch.no_grad():\n",
        "        state_tensor = state.unsqueeze(0) # batch dimension\n",
        "        q_values = model(state_tensor)\n",
        "\n",
        "        action = torch.argmax(\n",
        "                q_values.to('cpu').squeeze() + torch.from_numpy(mask) * 100., # trick to select a valid action\n",
        "                dim=0)\n",
        "\n",
        "    return action"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b99ac400",
      "metadata": {
        "id": "b99ac400"
      },
      "source": [
        "### Update 파트\n",
        "\n",
        "- Replay buffer 에서 batch하나를 샘플링하고,\n",
        "- model을 update한다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "e49c6175",
      "metadata": {
        "id": "e49c6175"
      },
      "outputs": [],
      "source": [
        "# sample a batch of _batch_size from replay buffers\n",
        "# return numpy.ndarrays\n",
        "def sample_batch(_batch_size):\n",
        "    # Get indices of samples for replay buffers\n",
        "    indices = np.random.choice(range(len(done_history)), size=_batch_size, replace=False)\n",
        "\n",
        "    state_sample = np.array([state_history[i].squeeze(0).numpy() for i in indices])\n",
        "    state_next_sample = np.array([state_next_history[i].squeeze(0).numpy() for i in indices])\n",
        "    rewards_sample = np.array([rewards_history[i] for i in indices], dtype=np.float32)\n",
        "    action_sample = np.array([action_history[i] for i in indices])\n",
        "\n",
        "    # action mask is the mask for the valid actions at the '''next''' state\n",
        "    action_mask_sample = np.array([action_mask_history[i] for i in indices])\n",
        "    done_sample = np.array([float(done_history[i]) for i in indices])\n",
        "\n",
        "    return state_sample, state_next_sample, rewards_sample, action_sample, action_mask_sample, done_sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "b97e7ba6",
      "metadata": {
        "id": "b97e7ba6"
      },
      "outputs": [],
      "source": [
        "# Function to update the Q-network\n",
        "def update_network():\n",
        "    # sample a batch of ...\n",
        "    state_sample, state_next_sample, rewards_sample, action_sample, action_mask_sample, done_sample = \\\n",
        "        sample_batch(batch_size)\n",
        "\n",
        "    # Convert numpy arrays to PyTorch tensors\n",
        "    state_sample = torch.tensor(state_sample, dtype=torch.float32).to(device)\n",
        "    state_next_sample = torch.tensor(state_next_sample, dtype=torch.float32).to(device)\n",
        "    action_sample = torch.tensor(action_sample, dtype=torch.int64).to(device)\n",
        "    action_mask_sample = torch.tensor(action_mask_sample, dtype=torch.int64).to(device)\n",
        "    rewards_sample = torch.tensor(rewards_sample, dtype=torch.float32).to(device)\n",
        "    done_sample = torch.tensor(done_sample, dtype=torch.float32).to(device)\n",
        "\n",
        "    # Compute the target Q-values for the states\n",
        "    with torch.no_grad():\n",
        "        future_rewards = model_target(state_next_sample)\n",
        "        #future_rewards = future_rewards.cpu()\n",
        "\n",
        "        # compute the q-value for the next state and the action maximizing the q-value\n",
        "        # note: the action should be valid (i.e., mask is set to 1)\n",
        "        max_q_values = torch.max(\n",
        "            future_rewards + action_mask_sample * 100., # trick to select a valid action\n",
        "            dim=1).values.detach() - 100.\n",
        "\n",
        "        # compute the target q-value\n",
        "        # if the step was final, max_q_values should not be added\n",
        "        # we assume that the negative return of the opposite player is the return of next step\n",
        "        # that is, G(t) = r(t+1) - g*r(t+2) + g^2*r(t+3) - g^3*r(t+4) + ...\n",
        "        target_q_values = rewards_sample + gamma * max_q_values * (1. - done_sample)\n",
        "\n",
        "    # It's forward propagation! Compute the Q-values for the taken actions\n",
        "    q_values = model(state_sample)\n",
        "    #q_values = q_values.cpu()\n",
        "    q_values_action = q_values.gather(dim=1, index=action_sample.unsqueeze(1)).squeeze(1)\n",
        "\n",
        "    # Compute the loss\n",
        "    loss = loss_function(q_values_action, target_q_values)\n",
        "\n",
        "    # Perform the optimization step\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "90ed4dcc-f24e-4990-879a-c2f6af51a063",
      "metadata": {
        "id": "90ed4dcc-f24e-4990-879a-c2f6af51a063"
      },
      "source": [
        "# Run DQN Tranining"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "43c0a265",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "43c0a265",
        "outputId": "8ca16c47-925a-4c15-9d53-7b5be56e3399"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode: 10, Frame count: 576, Running reward: -39.9\n",
            "Episode: 20, Frame count: 1141, Running reward: -38.9\n",
            "Episode: 30, Frame count: 1712, Running reward: -38.833333333333336\n",
            "Episode: 40, Frame count: 2294, Running reward: -39.075\n",
            "Episode: 50, Frame count: 2870, Running reward: -39.02\n",
            "Episode: 60, Frame count: 3453, Running reward: -39.13333333333333\n",
            "Episode: 70, Frame count: 4013, Running reward: -38.82857142857143\n",
            "Episode: 80, Frame count: 4595, Running reward: -39.025\n",
            "Episode: 90, Frame count: 5147, Running reward: -38.8\n",
            "Episode: 100, Frame count: 5728, Running reward: -38.93\n",
            "Episode: 110, Frame count: 6294, Running reward: -38.64\n",
            "Episode: 120, Frame count: 6864, Running reward: -38.67\n",
            "Episode: 130, Frame count: 7432, Running reward: -38.6\n",
            "Episode: 140, Frame count: 8003, Running reward: -38.57\n",
            "Episode: 150, Frame count: 8589, Running reward: -38.71\n",
            "Episode: 160, Frame count: 9166, Running reward: -38.63\n",
            "Episode: 170, Frame count: 9687, Running reward: -38.2\n",
            "Episode: 180, Frame count: 10267, Running reward: -38.12\n",
            "Episode: 190, Frame count: 10795, Running reward: -37.78\n",
            "Episode: 200, Frame count: 11379, Running reward: -37.81\n",
            "Episode: 210, Frame count: 11902, Running reward: -37.36\n",
            "Episode: 220, Frame count: 12463, Running reward: -37.29\n",
            "Episode: 230, Frame count: 13010, Running reward: -37.08\n",
            "Episode: 240, Frame count: 13561, Running reward: -36.72\n",
            "Episode: 250, Frame count: 14089, Running reward: -36.04\n",
            "Episode: 260, Frame count: 14652, Running reward: -35.9\n",
            "Episode: 270, Frame count: 15183, Running reward: -35.96\n",
            "Episode: 280, Frame count: 15737, Running reward: -35.6\n",
            "Episode: 290, Frame count: 16286, Running reward: -35.81\n",
            "Episode: 300, Frame count: 16818, Running reward: -35.17\n",
            "Episode: 310, Frame count: 17345, Running reward: -35.27\n",
            "Episode: 320, Frame count: 17865, Running reward: -34.74\n",
            "Episode: 330, Frame count: 18401, Running reward: -34.53\n",
            "Episode: 340, Frame count: 18932, Running reward: -34.37\n",
            "Episode: 350, Frame count: 19495, Running reward: -34.76\n",
            "Episode: 360, Frame count: 20056, Running reward: -34.74\n",
            "Episode: 370, Frame count: 20574, Running reward: -34.63\n",
            "Episode: 380, Frame count: 21096, Running reward: -34.27\n",
            "Episode: 390, Frame count: 21612, Running reward: -33.88\n",
            "Episode: 400, Frame count: 22127, Running reward: -33.67\n",
            "Episode: 410, Frame count: 22684, Running reward: -33.93\n",
            "Episode: 420, Frame count: 23197, Running reward: -33.88\n",
            "Episode: 430, Frame count: 23736, Running reward: -33.95\n",
            "Episode: 440, Frame count: 24252, Running reward: -33.68\n",
            "Episode: 450, Frame count: 24758, Running reward: -33.07\n",
            "Episode: 460, Frame count: 25217, Running reward: -31.93\n",
            "Episode: 470, Frame count: 25639, Running reward: -30.93\n",
            "Episode: 480, Frame count: 26159, Running reward: -30.95\n",
            "Episode: 490, Frame count: 26701, Running reward: -31.21\n",
            "Episode: 500, Frame count: 27210, Running reward: -31.13\n",
            "Episode: 510, Frame count: 27727, Running reward: -30.71\n",
            "Episode: 520, Frame count: 28202, Running reward: -30.29\n",
            "Episode: 530, Frame count: 28726, Running reward: -30.1\n",
            "Episode: 540, Frame count: 29198, Running reward: -29.68\n",
            "Episode: 550, Frame count: 29667, Running reward: -29.25\n",
            "Episode: 560, Frame count: 30122, Running reward: -29.21\n",
            "Episode: 570, Frame count: 30618, Running reward: -29.97\n",
            "Episode: 580, Frame count: 31148, Running reward: -30.03\n",
            "Episode: 590, Frame count: 31629, Running reward: -29.42\n",
            "Episode: 600, Frame count: 32081, Running reward: -28.85\n",
            "Episode: 610, Frame count: 32602, Running reward: -28.89\n",
            "Episode: 620, Frame count: 33068, Running reward: -28.8\n",
            "Episode: 630, Frame count: 33532, Running reward: -28.2\n",
            "Episode: 640, Frame count: 34029, Running reward: -28.45\n",
            "Episode: 650, Frame count: 34506, Running reward: -28.53\n",
            "Episode: 660, Frame count: 34996, Running reward: -28.92\n",
            "Episode: 670, Frame count: 35421, Running reward: -28.19\n",
            "Episode: 680, Frame count: 35893, Running reward: -27.61\n",
            "Episode: 690, Frame count: 36323, Running reward: -27.08\n",
            "Episode: 700, Frame count: 36760, Running reward: -26.91\n",
            "Episode: 710, Frame count: 37218, Running reward: -26.32\n",
            "Episode: 720, Frame count: 37687, Running reward: -26.37\n",
            "Episode: 730, Frame count: 38168, Running reward: -26.56\n",
            "Episode: 740, Frame count: 38639, Running reward: -26.3\n",
            "Episode: 750, Frame count: 39104, Running reward: -26.18\n",
            "Episode: 760, Frame count: 39621, Running reward: -26.45\n",
            "Episode: 770, Frame count: 40059, Running reward: -26.58\n",
            "Episode: 780, Frame count: 40514, Running reward: -26.39\n",
            "Episode: 790, Frame count: 40953, Running reward: -26.48\n",
            "Episode: 800, Frame count: 41408, Running reward: -26.68\n",
            "Episode: 810, Frame count: 41894, Running reward: -26.92\n",
            "Episode: 820, Frame count: 42386, Running reward: -27.19\n",
            "Episode: 830, Frame count: 42872, Running reward: -27.22\n",
            "Episode: 840, Frame count: 43364, Running reward: -27.45\n",
            "Episode: 850, Frame count: 43774, Running reward: -26.9\n",
            "Episode: 860, Frame count: 44238, Running reward: -26.33\n",
            "Episode: 870, Frame count: 44633, Running reward: -25.9\n",
            "Episode: 880, Frame count: 45120, Running reward: -26.22\n",
            "Episode: 890, Frame count: 45549, Running reward: -26.12\n",
            "Episode: 900, Frame count: 46006, Running reward: -26.14\n",
            "Episode: 910, Frame count: 46416, Running reward: -25.36\n",
            "Episode: 920, Frame count: 46775, Running reward: -23.97\n",
            "Episode: 930, Frame count: 47188, Running reward: -23.22\n",
            "Episode: 940, Frame count: 47622, Running reward: -22.62\n",
            "Episode: 950, Frame count: 48087, Running reward: -23.17\n",
            "Episode: 960, Frame count: 48533, Running reward: -23.01\n",
            "Episode: 970, Frame count: 48999, Running reward: -23.72\n",
            "Episode: 980, Frame count: 49432, Running reward: -23.2\n",
            "Episode: 990, Frame count: 49895, Running reward: -23.56\n",
            "Episode: 1000, Frame count: 50313, Running reward: -23.15\n",
            "Episode: 1010, Frame count: 50726, Running reward: -23.18\n",
            "Episode: 1020, Frame count: 51143, Running reward: -23.78\n",
            "Episode: 1030, Frame count: 51557, Running reward: -23.83\n",
            "Episode: 1040, Frame count: 51960, Running reward: -23.5\n",
            "Episode: 1050, Frame count: 52401, Running reward: -23.28\n",
            "Episode: 1060, Frame count: 52787, Running reward: -22.66\n",
            "Episode: 1070, Frame count: 53202, Running reward: -22.15\n",
            "Episode: 1080, Frame count: 53589, Running reward: -21.67\n",
            "Episode: 1090, Frame count: 54038, Running reward: -21.51\n",
            "Episode: 1100, Frame count: 54468, Running reward: -21.63\n",
            "Episode: 1110, Frame count: 54916, Running reward: -21.98\n",
            "Episode: 1120, Frame count: 55333, Running reward: -21.98\n",
            "Episode: 1130, Frame count: 55715, Running reward: -21.62\n",
            "Episode: 1140, Frame count: 56097, Running reward: -21.41\n",
            "Episode: 1150, Frame count: 56479, Running reward: -20.8\n",
            "Episode: 1160, Frame count: 56875, Running reward: -20.92\n",
            "Episode: 1170, Frame count: 57317, Running reward: -21.19\n",
            "Episode: 1180, Frame count: 57706, Running reward: -21.21\n",
            "Episode: 1190, Frame count: 58112, Running reward: -20.78\n",
            "Episode: 1200, Frame count: 58534, Running reward: -20.7\n",
            "Episode: 1210, Frame count: 58932, Running reward: -20.2\n",
            "Episode: 1220, Frame count: 59366, Running reward: -20.35\n",
            "Episode: 1230, Frame count: 59742, Running reward: -20.29\n",
            "Episode: 1240, Frame count: 60108, Running reward: -20.13\n",
            "Episode: 1250, Frame count: 60481, Running reward: -20.04\n",
            "Episode: 1260, Frame count: 60889, Running reward: -20.14\n",
            "Episode: 1270, Frame count: 61286, Running reward: -19.69\n",
            "Episode: 1280, Frame count: 61649, Running reward: -19.43\n",
            "Episode: 1290, Frame count: 62047, Running reward: -19.35\n",
            "Episode: 1300, Frame count: 62392, Running reward: -18.58\n",
            "Episode: 1310, Frame count: 62773, Running reward: -18.41\n",
            "Episode: 1320, Frame count: 63188, Running reward: -18.22\n",
            "Episode: 1330, Frame count: 63578, Running reward: -18.36\n",
            "Episode: 1340, Frame count: 63973, Running reward: -18.65\n",
            "Episode: 1350, Frame count: 64390, Running reward: -19.09\n",
            "Episode: 1360, Frame count: 64759, Running reward: -18.7\n",
            "Episode: 1370, Frame count: 65095, Running reward: -18.09\n",
            "Episode: 1380, Frame count: 65447, Running reward: -17.98\n",
            "Episode: 1390, Frame count: 65858, Running reward: -18.11\n",
            "Episode: 1400, Frame count: 66248, Running reward: -18.56\n",
            "Episode: 1410, Frame count: 66670, Running reward: -18.99\n",
            "Episode: 1420, Frame count: 67056, Running reward: -18.7\n",
            "Episode: 1430, Frame count: 67459, Running reward: -18.83\n",
            "Episode: 1440, Frame count: 67866, Running reward: -18.95\n",
            "Episode: 1450, Frame count: 68242, Running reward: -18.54\n",
            "Episode: 1460, Frame count: 68577, Running reward: -18.2\n",
            "Episode: 1470, Frame count: 68973, Running reward: -18.8\n",
            "Episode: 1480, Frame count: 69353, Running reward: -19.08\n",
            "Episode: 1490, Frame count: 69725, Running reward: -18.69\n",
            "Episode: 1500, Frame count: 70069, Running reward: -18.23\n",
            "Episode: 1510, Frame count: 70455, Running reward: -17.85\n",
            "Episode: 1520, Frame count: 70819, Running reward: -17.63\n",
            "Episode: 1530, Frame count: 71205, Running reward: -17.46\n",
            "Episode: 1540, Frame count: 71583, Running reward: -17.17\n",
            "Episode: 1550, Frame count: 71969, Running reward: -17.27\n",
            "Episode: 1560, Frame count: 72376, Running reward: -17.99\n",
            "Episode: 1570, Frame count: 72733, Running reward: -17.6\n",
            "Episode: 1580, Frame count: 73138, Running reward: -17.85\n",
            "Episode: 1590, Frame count: 73510, Running reward: -17.85\n",
            "Episode: 1600, Frame count: 73892, Running reward: -18.23\n",
            "Episode: 1610, Frame count: 74281, Running reward: -18.26\n",
            "Episode: 1620, Frame count: 74620, Running reward: -18.01\n",
            "Episode: 1630, Frame count: 74985, Running reward: -17.8\n",
            "Episode: 1640, Frame count: 75355, Running reward: -17.72\n",
            "Episode: 1650, Frame count: 75702, Running reward: -17.33\n",
            "Episode: 1660, Frame count: 76039, Running reward: -16.63\n",
            "Episode: 1670, Frame count: 76381, Running reward: -16.48\n",
            "Episode: 1680, Frame count: 76704, Running reward: -15.66\n",
            "Episode: 1690, Frame count: 77039, Running reward: -15.31\n",
            "Episode: 1700, Frame count: 77401, Running reward: -15.13\n",
            "Episode: 1710, Frame count: 77735, Running reward: -14.58\n",
            "Episode: 1720, Frame count: 78085, Running reward: -14.69\n",
            "Episode: 1730, Frame count: 78442, Running reward: -14.61\n",
            "Episode: 1740, Frame count: 78781, Running reward: -14.3\n",
            "Episode: 1750, Frame count: 79155, Running reward: -14.57\n",
            "Episode: 1760, Frame count: 79479, Running reward: -14.44\n",
            "Episode: 1770, Frame count: 79808, Running reward: -14.31\n",
            "Episode: 1780, Frame count: 80168, Running reward: -14.68\n",
            "Episode: 1790, Frame count: 80543, Running reward: -15.06\n",
            "Episode: 1800, Frame count: 80875, Running reward: -14.74\n",
            "Episode: 1810, Frame count: 81212, Running reward: -14.77\n",
            "Episode: 1820, Frame count: 81609, Running reward: -15.24\n",
            "Episode: 1830, Frame count: 81967, Running reward: -15.25\n",
            "Episode: 1840, Frame count: 82353, Running reward: -15.72\n",
            "Episode: 1850, Frame count: 82650, Running reward: -14.95\n",
            "Episode: 1860, Frame count: 83009, Running reward: -15.3\n",
            "Episode: 1870, Frame count: 83335, Running reward: -15.27\n",
            "Episode: 1880, Frame count: 83668, Running reward: -15.0\n",
            "Episode: 1890, Frame count: 83983, Running reward: -14.4\n",
            "Episode: 1900, Frame count: 84321, Running reward: -14.46\n",
            "Episode: 1910, Frame count: 84662, Running reward: -14.5\n",
            "Episode: 1920, Frame count: 85028, Running reward: -14.19\n",
            "Episode: 1930, Frame count: 85349, Running reward: -13.82\n",
            "Episode: 1940, Frame count: 85648, Running reward: -12.95\n",
            "Episode: 1950, Frame count: 85999, Running reward: -13.49\n",
            "Episode: 1960, Frame count: 86368, Running reward: -13.59\n",
            "Episode: 1970, Frame count: 86664, Running reward: -13.29\n",
            "Episode: 1980, Frame count: 87006, Running reward: -13.38\n",
            "Episode: 1990, Frame count: 87352, Running reward: -13.69\n",
            "Episode: 2000, Frame count: 87715, Running reward: -13.94\n",
            "Episode: 2010, Frame count: 88033, Running reward: -13.71\n",
            "Episode: 2020, Frame count: 88368, Running reward: -13.4\n",
            "Episode: 2030, Frame count: 88676, Running reward: -13.27\n",
            "Episode: 2040, Frame count: 89080, Running reward: -14.32\n",
            "Episode: 2050, Frame count: 89369, Running reward: -13.7\n",
            "Episode: 2060, Frame count: 89689, Running reward: -13.21\n",
            "Episode: 2070, Frame count: 89991, Running reward: -13.27\n",
            "Episode: 2080, Frame count: 90285, Running reward: -12.79\n",
            "Episode: 2090, Frame count: 90609, Running reward: -12.57\n",
            "Episode: 2100, Frame count: 90950, Running reward: -12.35\n",
            "Episode: 2110, Frame count: 91271, Running reward: -12.38\n",
            "Episode: 2120, Frame count: 91591, Running reward: -12.23\n",
            "Episode: 2130, Frame count: 91924, Running reward: -12.48\n",
            "Episode: 2140, Frame count: 92248, Running reward: -11.68\n",
            "Episode: 2150, Frame count: 92605, Running reward: -12.36\n",
            "Episode: 2160, Frame count: 92923, Running reward: -12.34\n",
            "Episode: 2170, Frame count: 93208, Running reward: -12.17\n",
            "Episode: 2180, Frame count: 93533, Running reward: -12.48\n",
            "Episode: 2190, Frame count: 93858, Running reward: -12.49\n",
            "Episode: 2200, Frame count: 94176, Running reward: -12.26\n",
            "Episode: 2210, Frame count: 94502, Running reward: -12.31\n",
            "Episode: 2220, Frame count: 94797, Running reward: -12.06\n",
            "Episode: 2230, Frame count: 95101, Running reward: -11.77\n",
            "Episode: 2240, Frame count: 95452, Running reward: -12.04\n",
            "Episode: 2250, Frame count: 95745, Running reward: -11.4\n",
            "Episode: 2260, Frame count: 96034, Running reward: -11.11\n",
            "Episode: 2270, Frame count: 96340, Running reward: -11.32\n",
            "Episode: 2280, Frame count: 96670, Running reward: -11.37\n",
            "Episode: 2290, Frame count: 96968, Running reward: -11.1\n",
            "Episode: 2300, Frame count: 97298, Running reward: -11.22\n",
            "Episode: 2310, Frame count: 97625, Running reward: -11.23\n",
            "Episode: 2320, Frame count: 97938, Running reward: -11.41\n",
            "Episode: 2330, Frame count: 98274, Running reward: -11.73\n",
            "Episode: 2340, Frame count: 98598, Running reward: -11.46\n",
            "Episode: 2350, Frame count: 98903, Running reward: -11.58\n",
            "Episode: 2360, Frame count: 99192, Running reward: -11.58\n",
            "Episode: 2370, Frame count: 99503, Running reward: -11.63\n",
            "Episode: 2380, Frame count: 99817, Running reward: -11.47\n",
            "Episode: 2390, Frame count: 100111, Running reward: -11.43\n",
            "Episode: 2400, Frame count: 100397, Running reward: -10.99\n",
            "Episode: 2410, Frame count: 100701, Running reward: -10.76\n",
            "Episode: 2420, Frame count: 100989, Running reward: -10.51\n",
            "Episode: 2430, Frame count: 101253, Running reward: -9.79\n",
            "Episode: 2440, Frame count: 101526, Running reward: -9.28\n",
            "Episode: 2450, Frame count: 101880, Running reward: -9.77\n",
            "Episode: 2460, Frame count: 102221, Running reward: -10.29\n",
            "Episode: 2470, Frame count: 102516, Running reward: -10.13\n",
            "Episode: 2480, Frame count: 102827, Running reward: -10.1\n",
            "Episode: 2490, Frame count: 103128, Running reward: -10.17\n",
            "Episode: 2500, Frame count: 103458, Running reward: -10.61\n",
            "Episode: 2510, Frame count: 103794, Running reward: -10.93\n",
            "Episode: 2520, Frame count: 104093, Running reward: -11.04\n",
            "Episode: 2530, Frame count: 104334, Running reward: -10.81\n",
            "Episode: 2540, Frame count: 104661, Running reward: -11.35\n",
            "Episode: 2550, Frame count: 104943, Running reward: -10.63\n",
            "Episode: 2560, Frame count: 105223, Running reward: -10.02\n",
            "Episode: 2570, Frame count: 105564, Running reward: -10.48\n",
            "Episode: 2580, Frame count: 105826, Running reward: -9.99\n",
            "Episode: 2590, Frame count: 106119, Running reward: -9.91\n",
            "Episode: 2600, Frame count: 106420, Running reward: -9.62\n",
            "Episode: 2610, Frame count: 106699, Running reward: -9.05\n",
            "Episode: 2620, Frame count: 106997, Running reward: -9.04\n",
            "Episode: 2630, Frame count: 107279, Running reward: -9.45\n",
            "Episode: 2640, Frame count: 107603, Running reward: -9.42\n",
            "Episode: 2650, Frame count: 107892, Running reward: -9.49\n",
            "Episode: 2660, Frame count: 108220, Running reward: -9.97\n",
            "Episode: 2670, Frame count: 108543, Running reward: -9.79\n",
            "Episode: 2680, Frame count: 108850, Running reward: -10.24\n",
            "Episode: 2690, Frame count: 109141, Running reward: -10.22\n",
            "Episode: 2700, Frame count: 109402, Running reward: -9.82\n",
            "Episode: 2710, Frame count: 109680, Running reward: -9.81\n",
            "Episode: 2720, Frame count: 109974, Running reward: -9.77\n",
            "Episode: 2730, Frame count: 110227, Running reward: -9.48\n",
            "Episode: 2740, Frame count: 110523, Running reward: -9.2\n",
            "Episode: 2750, Frame count: 110821, Running reward: -9.29\n",
            "Episode: 2760, Frame count: 111130, Running reward: -9.1\n",
            "Episode: 2770, Frame count: 111431, Running reward: -8.88\n",
            "Episode: 2780, Frame count: 111734, Running reward: -8.84\n",
            "Episode: 2790, Frame count: 112048, Running reward: -9.07\n",
            "Episode: 2800, Frame count: 112317, Running reward: -9.15\n",
            "Episode: 2810, Frame count: 112629, Running reward: -9.49\n",
            "Episode: 2820, Frame count: 112887, Running reward: -9.13\n",
            "Episode: 2830, Frame count: 113172, Running reward: -9.45\n",
            "Episode: 2840, Frame count: 113444, Running reward: -9.21\n",
            "Episode: 2850, Frame count: 113725, Running reward: -9.04\n",
            "Episode: 2860, Frame count: 114007, Running reward: -8.77\n",
            "Episode: 2870, Frame count: 114261, Running reward: -8.3\n",
            "Episode: 2880, Frame count: 114561, Running reward: -8.27\n",
            "Episode: 2890, Frame count: 114900, Running reward: -8.52\n",
            "Episode: 2900, Frame count: 115207, Running reward: -8.9\n",
            "Episode: 2910, Frame count: 115505, Running reward: -8.76\n",
            "Episode: 2920, Frame count: 115793, Running reward: -9.06\n",
            "Episode: 2930, Frame count: 116061, Running reward: -8.89\n",
            "Episode: 2940, Frame count: 116360, Running reward: -9.16\n",
            "Episode: 2950, Frame count: 116623, Running reward: -8.98\n",
            "Episode: 2960, Frame count: 116872, Running reward: -8.65\n",
            "Episode: 2970, Frame count: 117186, Running reward: -9.25\n",
            "Episode: 2980, Frame count: 117442, Running reward: -8.81\n",
            "Episode: 2990, Frame count: 117705, Running reward: -8.05\n",
            "Episode: 3000, Frame count: 117970, Running reward: -7.63\n",
            "Episode: 3010, Frame count: 118213, Running reward: -7.08\n",
            "Episode: 3020, Frame count: 118470, Running reward: -6.77\n",
            "Episode: 3030, Frame count: 118739, Running reward: -6.78\n",
            "Episode: 3040, Frame count: 118975, Running reward: -6.15\n",
            "Episode: 3050, Frame count: 119243, Running reward: -6.2\n",
            "Episode: 3060, Frame count: 119505, Running reward: -6.33\n",
            "Episode: 3070, Frame count: 119792, Running reward: -6.06\n",
            "Episode: 3080, Frame count: 120056, Running reward: -6.14\n",
            "Episode: 3090, Frame count: 120290, Running reward: -5.85\n",
            "Episode: 3100, Frame count: 120540, Running reward: -5.7\n",
            "Episode: 3110, Frame count: 120800, Running reward: -5.87\n",
            "Episode: 3120, Frame count: 121026, Running reward: -5.56\n",
            "Episode: 3130, Frame count: 121272, Running reward: -5.33\n",
            "Episode: 3140, Frame count: 121520, Running reward: -5.45\n",
            "Episode: 3150, Frame count: 121798, Running reward: -5.55\n",
            "Episode: 3160, Frame count: 122056, Running reward: -5.51\n",
            "Episode: 3170, Frame count: 122317, Running reward: -5.25\n",
            "Episode: 3180, Frame count: 122593, Running reward: -5.37\n",
            "Episode: 3190, Frame count: 122868, Running reward: -5.78\n",
            "Episode: 3200, Frame count: 123147, Running reward: -6.07\n",
            "Episode: 3210, Frame count: 123427, Running reward: -6.27\n",
            "Episode: 3220, Frame count: 123707, Running reward: -6.81\n",
            "Episode: 3230, Frame count: 123981, Running reward: -7.09\n",
            "Episode: 3240, Frame count: 124231, Running reward: -7.11\n",
            "Episode: 3250, Frame count: 124495, Running reward: -6.97\n",
            "Episode: 3260, Frame count: 124753, Running reward: -6.97\n",
            "Episode: 3270, Frame count: 125010, Running reward: -6.93\n",
            "Episode: 3280, Frame count: 125297, Running reward: -7.04\n",
            "Episode: 3290, Frame count: 125531, Running reward: -6.63\n",
            "Episode: 3300, Frame count: 125778, Running reward: -6.31\n",
            "Episode: 3310, Frame count: 126040, Running reward: -6.13\n",
            "Episode: 3320, Frame count: 126314, Running reward: -6.07\n",
            "Episode: 3330, Frame count: 126607, Running reward: -6.26\n",
            "Episode: 3340, Frame count: 126879, Running reward: -6.48\n",
            "Episode: 3350, Frame count: 127145, Running reward: -6.5\n",
            "Episode: 3360, Frame count: 127421, Running reward: -6.68\n",
            "Episode: 3370, Frame count: 127673, Running reward: -6.63\n",
            "Episode: 3380, Frame count: 127915, Running reward: -6.18\n",
            "Episode: 3390, Frame count: 128171, Running reward: -6.4\n",
            "Episode: 3400, Frame count: 128439, Running reward: -6.61\n",
            "Episode: 3410, Frame count: 128706, Running reward: -6.66\n",
            "Episode: 3420, Frame count: 128925, Running reward: -6.11\n",
            "Episode: 3430, Frame count: 129177, Running reward: -5.7\n",
            "Episode: 3440, Frame count: 129397, Running reward: -5.18\n",
            "Episode: 3450, Frame count: 129664, Running reward: -5.19\n",
            "Episode: 3460, Frame count: 129896, Running reward: -4.75\n",
            "Episode: 3470, Frame count: 130148, Running reward: -4.75\n",
            "Episode: 3480, Frame count: 130395, Running reward: -4.8\n",
            "Episode: 3490, Frame count: 130623, Running reward: -4.52\n",
            "Episode: 3500, Frame count: 130894, Running reward: -4.55\n",
            "Episode: 3510, Frame count: 131150, Running reward: -4.44\n",
            "Episode: 3520, Frame count: 131369, Running reward: -4.44\n",
            "Episode: 3530, Frame count: 131630, Running reward: -4.53\n",
            "Episode: 3540, Frame count: 131892, Running reward: -4.95\n",
            "Episode: 3550, Frame count: 132129, Running reward: -4.65\n",
            "Episode: 3560, Frame count: 132373, Running reward: -4.77\n",
            "Episode: 3570, Frame count: 132622, Running reward: -4.74\n",
            "Episode: 3580, Frame count: 132847, Running reward: -4.52\n",
            "Episode: 3590, Frame count: 133060, Running reward: -4.37\n",
            "Episode: 3600, Frame count: 133302, Running reward: -4.08\n",
            "Episode: 3610, Frame count: 133538, Running reward: -3.88\n",
            "Episode: 3620, Frame count: 133767, Running reward: -3.98\n",
            "Episode: 3630, Frame count: 134032, Running reward: -4.02\n",
            "Episode: 3640, Frame count: 134315, Running reward: -4.23\n",
            "Episode: 3650, Frame count: 134548, Running reward: -4.19\n",
            "Episode: 3660, Frame count: 134774, Running reward: -4.01\n",
            "Episode: 3670, Frame count: 134991, Running reward: -3.69\n",
            "Episode: 3680, Frame count: 135226, Running reward: -3.79\n",
            "Episode: 3690, Frame count: 135475, Running reward: -4.15\n",
            "Episode: 3700, Frame count: 135724, Running reward: -4.22\n",
            "Episode: 3710, Frame count: 135980, Running reward: -4.42\n",
            "Episode: 3720, Frame count: 136235, Running reward: -4.68\n",
            "Episode: 3730, Frame count: 136520, Running reward: -4.88\n",
            "Episode: 3740, Frame count: 136740, Running reward: -4.25\n",
            "Episode: 3750, Frame count: 136982, Running reward: -4.34\n",
            "Episode: 3760, Frame count: 137224, Running reward: -4.5\n",
            "Episode: 3770, Frame count: 137468, Running reward: -4.77\n",
            "Episode: 3780, Frame count: 137717, Running reward: -4.91\n",
            "Episode: 3790, Frame count: 137946, Running reward: -4.71\n",
            "Episode: 3800, Frame count: 138149, Running reward: -4.25\n",
            "Episode: 3810, Frame count: 138412, Running reward: -4.32\n",
            "Episode: 3820, Frame count: 138645, Running reward: -4.1\n",
            "Episode: 3830, Frame count: 138875, Running reward: -3.55\n",
            "Episode: 3840, Frame count: 139107, Running reward: -3.67\n",
            "Episode: 3850, Frame count: 139365, Running reward: -3.83\n",
            "Episode: 3860, Frame count: 139603, Running reward: -3.79\n",
            "Episode: 3870, Frame count: 139827, Running reward: -3.59\n",
            "Episode: 3880, Frame count: 140039, Running reward: -3.22\n",
            "Episode: 3890, Frame count: 140269, Running reward: -3.23\n",
            "Episode: 3900, Frame count: 140517, Running reward: -3.68\n",
            "Episode: 3910, Frame count: 140745, Running reward: -3.33\n",
            "Episode: 3920, Frame count: 140976, Running reward: -3.31\n",
            "Episode: 3930, Frame count: 141201, Running reward: -3.26\n",
            "Episode: 3940, Frame count: 141429, Running reward: -3.22\n",
            "Episode: 3950, Frame count: 141646, Running reward: -2.81\n",
            "Episode: 3960, Frame count: 141853, Running reward: -2.5\n",
            "Episode: 3970, Frame count: 142075, Running reward: -2.48\n",
            "Episode: 3980, Frame count: 142288, Running reward: -2.49\n",
            "Episode: 3990, Frame count: 142511, Running reward: -2.42\n",
            "Episode: 4000, Frame count: 142735, Running reward: -2.18\n",
            "Episode: 4010, Frame count: 142990, Running reward: -2.45\n",
            "Episode: 4020, Frame count: 143213, Running reward: -2.37\n",
            "Episode: 4030, Frame count: 143448, Running reward: -2.47\n",
            "Episode: 4040, Frame count: 143665, Running reward: -2.36\n",
            "Episode: 4050, Frame count: 143888, Running reward: -2.42\n",
            "Episode: 4060, Frame count: 144122, Running reward: -2.69\n",
            "Episode: 4070, Frame count: 144335, Running reward: -2.6\n",
            "Episode: 4080, Frame count: 144544, Running reward: -2.56\n",
            "Episode: 4090, Frame count: 144782, Running reward: -2.71\n",
            "Episode: 4100, Frame count: 145019, Running reward: -2.84\n",
            "Episode: 4110, Frame count: 145213, Running reward: -2.23\n",
            "Episode: 4120, Frame count: 145427, Running reward: -2.14\n",
            "Episode: 4130, Frame count: 145649, Running reward: -2.01\n",
            "Episode: 4140, Frame count: 145868, Running reward: -2.03\n",
            "Episode: 4150, Frame count: 146087, Running reward: -1.99\n",
            "Episode: 4160, Frame count: 146304, Running reward: -1.82\n",
            "Episode: 4170, Frame count: 146509, Running reward: -1.74\n",
            "Episode: 4180, Frame count: 146771, Running reward: -2.27\n",
            "Episode: 4190, Frame count: 147018, Running reward: -2.36\n",
            "Episode: 4200, Frame count: 147273, Running reward: -2.54\n",
            "Episode: 4210, Frame count: 147526, Running reward: -3.13\n",
            "Episode: 4220, Frame count: 147749, Running reward: -3.22\n",
            "Episode: 4230, Frame count: 147985, Running reward: -3.36\n",
            "Episode: 4240, Frame count: 148208, Running reward: -3.4\n",
            "Episode: 4250, Frame count: 148421, Running reward: -3.34\n",
            "Episode: 4260, Frame count: 148656, Running reward: -3.52\n",
            "Episode: 4270, Frame count: 148897, Running reward: -3.88\n",
            "Episode: 4280, Frame count: 149105, Running reward: -3.34\n",
            "Episode: 4290, Frame count: 149338, Running reward: -3.2\n",
            "Episode: 4300, Frame count: 149568, Running reward: -2.95\n",
            "Episode: 4310, Frame count: 149805, Running reward: -2.79\n",
            "Episode: 4320, Frame count: 149993, Running reward: -2.44\n",
            "Episode: 4330, Frame count: 150224, Running reward: -2.39\n",
            "Episode: 4340, Frame count: 150419, Running reward: -2.11\n",
            "Episode: 4350, Frame count: 150657, Running reward: -2.36\n",
            "Episode: 4360, Frame count: 150873, Running reward: -2.17\n",
            "Episode: 4370, Frame count: 151092, Running reward: -1.95\n",
            "Episode: 4380, Frame count: 151315, Running reward: -2.1\n",
            "Episode: 4390, Frame count: 151516, Running reward: -1.78\n",
            "Episode: 4400, Frame count: 151732, Running reward: -1.64\n",
            "Episode: 4410, Frame count: 151983, Running reward: -1.78\n",
            "Episode: 4420, Frame count: 152210, Running reward: -2.17\n",
            "Episode: 4430, Frame count: 152444, Running reward: -2.2\n",
            "Episode: 4440, Frame count: 152634, Running reward: -2.15\n",
            "Episode: 4450, Frame count: 152863, Running reward: -2.06\n",
            "Episode: 4460, Frame count: 153093, Running reward: -2.2\n",
            "Episode: 4470, Frame count: 153299, Running reward: -2.07\n",
            "Episode: 4480, Frame count: 153497, Running reward: -1.82\n",
            "Episode: 4490, Frame count: 153713, Running reward: -1.97\n",
            "Episode: 4500, Frame count: 153923, Running reward: -1.91\n",
            "Episode: 4510, Frame count: 154121, Running reward: -1.38\n",
            "Episode: 4520, Frame count: 154344, Running reward: -1.34\n",
            "Episode: 4530, Frame count: 154548, Running reward: -1.04\n",
            "Episode: 4540, Frame count: 154761, Running reward: -1.27\n",
            "Episode: 4550, Frame count: 154964, Running reward: -1.01\n",
            "Episode: 4560, Frame count: 155183, Running reward: -0.9\n",
            "Episode: 4570, Frame count: 155400, Running reward: -1.01\n",
            "Episode: 4580, Frame count: 155622, Running reward: -1.25\n",
            "Episode: 4590, Frame count: 155835, Running reward: -1.22\n",
            "Episode: 4600, Frame count: 156042, Running reward: -1.19\n",
            "Episode: 4610, Frame count: 156248, Running reward: -1.27\n",
            "Episode: 4620, Frame count: 156476, Running reward: -1.32\n",
            "Episode: 4630, Frame count: 156684, Running reward: -1.36\n",
            "Episode: 4640, Frame count: 156879, Running reward: -1.18\n",
            "Episode: 4650, Frame count: 157063, Running reward: -0.99\n",
            "Episode: 4660, Frame count: 157272, Running reward: -0.89\n",
            "Episode: 4670, Frame count: 157488, Running reward: -0.88\n",
            "Episode: 4680, Frame count: 157691, Running reward: -0.69\n",
            "Episode: 4690, Frame count: 157882, Running reward: -0.47\n",
            "Episode: 4700, Frame count: 158075, Running reward: -0.33\n",
            "Episode: 4710, Frame count: 158275, Running reward: -0.27\n",
            "Episode: 4720, Frame count: 158491, Running reward: -0.15\n",
            "Episode: 4730, Frame count: 158711, Running reward: -0.27\n",
            "Episode: 4740, Frame count: 158913, Running reward: -0.34\n",
            "Episode: 4750, Frame count: 159136, Running reward: -0.73\n",
            "Episode: 4760, Frame count: 159352, Running reward: -0.8\n",
            "Episode: 4770, Frame count: 159576, Running reward: -0.88\n",
            "Episode: 4780, Frame count: 159790, Running reward: -0.99\n",
            "Episode: 4790, Frame count: 160009, Running reward: -1.27\n",
            "Episode: 4800, Frame count: 160241, Running reward: -1.66\n",
            "Episode: 4810, Frame count: 160437, Running reward: -1.62\n",
            "Episode: 4820, Frame count: 160646, Running reward: -1.55\n",
            "Episode: 4830, Frame count: 160864, Running reward: -1.53\n",
            "Episode: 4840, Frame count: 161078, Running reward: -1.65\n",
            "Episode: 4850, Frame count: 161280, Running reward: -1.44\n",
            "Episode: 4860, Frame count: 161485, Running reward: -1.33\n",
            "Episode: 4870, Frame count: 161694, Running reward: -1.18\n",
            "Episode: 4880, Frame count: 161877, Running reward: -0.87\n",
            "Episode: 4890, Frame count: 162089, Running reward: -0.8\n",
            "Episode: 4900, Frame count: 162294, Running reward: -0.53\n",
            "Episode: 4910, Frame count: 162489, Running reward: -0.52\n",
            "Episode: 4920, Frame count: 162690, Running reward: -0.44\n",
            "Episode: 4930, Frame count: 162883, Running reward: -0.19\n",
            "Episode: 4940, Frame count: 163092, Running reward: -0.14\n",
            "Episode: 4950, Frame count: 163294, Running reward: -0.14\n",
            "Episode: 4960, Frame count: 163489, Running reward: -0.04\n",
            "Episode: 4970, Frame count: 163664, Running reward: 0.3\n",
            "Episode: 4980, Frame count: 163860, Running reward: 0.17\n",
            "Episode: 4990, Frame count: 164059, Running reward: 0.3\n",
            "Episode: 5000, Frame count: 164262, Running reward: 0.32\n"
          ]
        }
      ],
      "source": [
        "for _ in range(max_episodes):\n",
        "    state, info = env.reset()\n",
        "    state = preprocess_state(state)\n",
        "    action_mask = info['action_mask'].reshape((-1,))\n",
        "    episode_reward = 0\n",
        "\n",
        "    for timestep in range(1, max_steps_per_episode):\n",
        "        frame_count += 1\n",
        "\n",
        "        # Select an action\n",
        "        #state_cuda = state.to(device)\n",
        "        action = get_greedy_epsilon(model,\n",
        "                      state.to(device),\n",
        "                      action_mask)\n",
        "        if action < 0:\n",
        "            print(action_mask)\n",
        "\n",
        "        # Take the selected action\n",
        "        state_next, reward, done,_, info = env.step((action // 8, action % 8))\n",
        "        state_next = preprocess_state(state_next)\n",
        "        action_mask = info['action_mask'].reshape((-1,))\n",
        "\n",
        "        episode_reward += reward\n",
        "\n",
        "        # Store the transition in the replay buffer\n",
        "        action_history.append(action)\n",
        "        action_mask_history.append(action_mask)\n",
        "        state_history.append(state)\n",
        "        state_next_history.append(state_next)\n",
        "        rewards_history.append(reward)\n",
        "        done_history.append(done)\n",
        "\n",
        "        state = state_next\n",
        "\n",
        "        # Update every fourth frame and once batch size is over 32\n",
        "        if frame_count % update_after_actions == 0 and len(done_history) > batch_size:\n",
        "            update_network()\n",
        "\n",
        "        if frame_count % update_target_network == 0:\n",
        "            model_target.load_state_dict(model.state_dict())\n",
        "\n",
        "        # Limit the state and reward history\n",
        "        if len(rewards_history) > max_memory_length:\n",
        "            del rewards_history[:1]\n",
        "            del state_history[:1]\n",
        "            del state_next_history[:1]\n",
        "            del action_history[:1]\n",
        "            del action_mask_history[:1]\n",
        "            del done_history[:1]\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    episode_count += 1\n",
        "    episode_reward_history.append(episode_reward)\n",
        "\n",
        "    # Update running reward to check condition for solving\n",
        "    if len(episode_reward_history) > 100:\n",
        "        del episode_reward_history[:1]\n",
        "    running_reward = np.mean(episode_reward_history)\n",
        "\n",
        "    if episode_count % 10 == 0:\n",
        "        print(f\"Episode: {episode_count}, Frame count: {frame_count}, Running reward: {running_reward}\")\n",
        "\n",
        "    if episode_count % 5000 == 0:\n",
        "        torch.save(model, 'model.{}'.format(episode_count))\n",
        "    #if running_reward > 20:\n",
        "    #    print(f\"Solved at episode {episode_count}!\")\n",
        "    #    break\n",
        "\n",
        "\n",
        "torch.save(model, 'model.final')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4984b880-e427-48cb-bf91-13a91d6529f5",
      "metadata": {
        "id": "4984b880-e427-48cb-bf91-13a91d6529f5"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "5c198b4e-b0e4-4fd4-821d-be602c2dbc5a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5c198b4e-b0e4-4fd4-821d-be602c2dbc5a",
        "outputId": "60b8b3d6-9976-40dc-82c2-2ff26e6ac94b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      H  |       H \n",
            "    H H  |     H H \n",
            "  MMHHHH |     HHHH\n",
            "    HMH  |     H H \n",
            "  M M H  |       H \n",
            "    M    |         \n",
            "         |         \n",
            "         |         \n",
            "\n"
          ]
        }
      ],
      "source": [
        "import time, sys\n",
        "from IPython.display import clear_output\n",
        "\n",
        "board, info = env.reset()\n",
        "state = preprocess_state(board)\n",
        "action_mask = info['action_mask'].reshape((-1,))\n",
        "done = False\n",
        "env.render()\n",
        "\n",
        "while not done:\n",
        "    action = get_greedy_action(model, state.to(device), action_mask)\n",
        "    print(\"action: ({}, {})\".format(action // 8, action % 8))\n",
        "    sys.stdout.flush()\n",
        "\n",
        "    time.sleep(1.0)\n",
        "    clear_output(wait=False)\n",
        "    board, reward, done,_, info = env.step((action // 8, action % 8))\n",
        "    state = preprocess_state(board)\n",
        "    action_mask = info['action_mask'].reshape((-1,))\n",
        "    env.render()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}